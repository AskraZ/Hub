{"Algebra-Lineare":{"slug":"Algebra-Lineare","filePath":"Algebra Lineare.md","title":"Algebra Lineare","links":["Drawing-2025-10-08-21.09.51.excalidraw","Drawing-2025-10-08-22.08.48.excalidraw","tags/SOLUZIONE"],"tags":["SOLUZIONE"],"content":"Indice\nStrutture Algebriche\nMatrici\nSpazi Vettoriali\nApplicazioni Lineari\nStrutture Algebriche\nGli insiemi numerici principali sono i seguenti:\nZ = \\{0, \\pm1,\\pm2,\\pm3,...\\pm n\\}\nQ=\\{\\frac{m}{n}| m \\in Z , n\\in Z, n \\neq0\\}\nR=\\{M,C_1,C_2....C_{\\infty}|M\\in Z, C_{i}\\in Z, 0 \\leq C_{i} \\leq 9\\}\nNoi sappiamo che su questi insiemi possiamo fare due operazioni:\n\nSOMMA → 2 + 3 = 5\nPRODOTTO → 2 \\times 3 = 6\nQuindi, un’operazione è una funzione che ad una coppia di numeri associa un altro numero.\n\nDEFINIZIONE (PRODOTTO CARTESIANO)\nsiano A \\ e \\ B  due insiemi; si chiama prodotto cartesiano di A per B, L’insieme:\nA \\ X \\ B \\ = \\{ (a,b)| \\ a \\in A, \\ b \\in B\\}\nESEMPIO\nsia  A= \\{ 1,2 \\}, \\ B= \\{2,3,4 \\}\nA \\ X \\ B = \\{(1,2),(1,3),(1,4),(2,2),(2,3),(2,4) \\}\nDEFINIZIONE (OPERAZIONE BINARIA INTERNA)\nsia A un insieme qualunque.\nsi chiama operazione binaria interna su A una funzione\nA \\ X \\ A \\to A\nNOTAZIONE\nsia (a,b \\in  A \\ X \\ A; \\ f((a,b)) \\in A)\nindicheremo l’operazione con * e scriveremo:\na*b=f((a,b))\nDEFINIZIONE (STRUTTURA ALGEBRICA)\nsi chiama struttura algebrica un insieme su cui sono definite una o più operazioni\nDEFINIZIONE (GRUPPOIDE)\nsi chiama Gruppoide una struttura algebrica costituita da un insieme A e da una operazione binaria interna su A, e scriviamo (A,*)\nDEFINIZIONE (PROPRIETA’ ASSOCIATIVA)\nsia (A,*) un gruppoide.\ndiremo che l’operazione * gode della proprietà associativa se\n(a*b)*c=a*(b*c) , \\forall a,b,c \\in A\nESEMPIO\n(2 \\times 3)\\times 4=6 \\times 4=24 \\equiv 2 \\times (3 \\times 4)=2 \\times 12 = 24\n(2+3)+4=5+4=9 \\equiv 2+(3+4)=2+7=9\nDEFINIZIONE (SEMIGRUPPO)\nsia (A,*) un gruppoide.\ndiremo che (A,*) è un semigruppo se l’operazione è associativa.\nDEFINIZIONE (ELEMENTO NEUTRO)\nsia (A,*) un gruppoide.\nsia \\epsilon \\in A, diremo che che \\epsilon è un elemento neutro se\na*\\epsilon =\\epsilon*a=a,   \\forall a \\in A\nPROPOSIZIONE\nsia (A,*) un gruppoide.\nsiano \\epsilon ,\\epsilon ^I \\in A.\nelementi neutri \\rightarrow \\epsilon = \\epsilon ^I\nDIMOSTRAZIONE\n\\epsilon*\\epsilon ^I=\\epsilon                          \\epsilon ^I*\\epsilon=\\epsilon^I\nquindi \\epsilon=\\epsilon ^I\nDEFINIZIONE (MONOIDE)\nsia (A,*) un semigruppo.\ndiremo che esso è un monoide se esiste l’elemento neutro.\nESEMPIO\n(Z, +) è un monoide con elemento neutro 0.\n(Z, \\times) è un monoide con elemento neutro 1.\nDEFINIZIONE (INVERTIBILITA’)\nsia (A,*) un monoide, con elemento neutro \\epsilon .\nsia a \\in A  diremo che a è invertibile se \\exists a^{*}\\in A , tale che\na*a^{*}=a^{*}*a=\\epsilon\nin questo caso diremo che a^* è un inverso di a.\nESEMPIO\n(1) consideriamo il monoide (Z, +) sia 2 \\in Z\n2 è invertibile.\ninfatti 2+(-2)=(-2)+2=0\n(2) consideriamo il monoide (Z, \\times) sia 2 \\in Z\n2 non è invertibile.\n(3) consideriamo il monoide (Q, \\times), sia 2 \\in Q\n2 è invertibile.\ninfatti 2 \\times \\frac{1}{2}=\\frac{1}{2}\\times 2=1\nPROPOSIZIONE\nsia (A,*) un monoide. sia a \\in A, a invertibile \\rightarrow a ha solo un inverso.\nDIMOSTRAZIONE\nsiano a^{*}\\in A e a^{**}\\in A due inversi di A.\n\n(a^{*}* a)*a^{**}=a^{*}*(a*a^{**})\n(a^{*}*a)*a^{**}=\\epsilon * a^{**}=a^{**}\n(a^{**}*a)*a^{*}=\\epsilon * a^{*}=a^{*}\nsi ricava che a^{*}=a^{**}\n\nPROPOSIZIONE\nsia (A, *) un monoide. siano a \\in A, b \\in A, due elementi invertibili.\nsiano a^{-1} e b^-1 gli inversi di a e b rispettivamente.\nallora a*b è invertibile e\n(a*b)^-1=b^{-1}*a^{-1}\nDIMOSTRAZIONE\nvogliamo dimostrare che b^{-1}*a^-1 è l’inverso di a*b .\ninfatti:\n\n(a*b)*(b^{-1}*a^{-1})=a*(b*b^{-1})*a^{-1} \\ =(a*\\epsilon)*a^-1=a*a^-1=\\epsilon\n(b^{-1}*a^{-1})*(a*b) =b^{-1}*(a^{-1}*a)*b=(b^{-1}*\\epsilon)*b=b^{-1}*b=\\epsilon\n\nNOTAZIONE\nsia (A,*) un monoide, sia a \\in A invertibile.\ndenoteremo l’inverso di a con il simbolo a^{-1}\nDEFINIZIONE (GRUPPO)\nsi chiama gruppo un monoide nel quale ogni elemento è invertibile.\nDEFINIZIONE (PROPRIETA’ COMMUTATIVA)\nsia (A,*) un gruppoide.\ndiremo che l’operazione * gode della proprietà commutativa se\na*b=b*a  \\forall a,b \\in A\nDEFINIZIONE (GRUPPO ABELIANO)\nun gruppo (A,*) si dice Abeliano se l’operazione è commutativa.\nESEMPIO\n\n(Z, +) è un gruppo abeliano\n(Z, \\times) è un monoide, ma non un gruppo\n(Q, \\times) è un monoide, ma non è un gruppo\n(Q \\{0\\}, \\times) è un gruppo abeliano\n(R\\{0\\}, \\times) è un gruppo abeliano\n(Q, +) è un gruppo abeliano\n(R,+) è un gruppo abeliano\n\nDEFINIZIONE (ANELLO)\nsia (A,*, \\times) una struttura algebrica.\ndiremo che essa è un anello se valgono le seguenti proprietà:\n\n(A,+) è un gruppo abeliano\n(A, \\times) è un semigruppo\nproprietà distributiva rispetto alla somma\n\nNOTAZIONE\nsia (A,+,\\times) un anello.\nl’elemento neutro della somma lo indichiamo con 0 e lo chiamiamo zero.\n\\forall a \\in A, il suo inverso rispetto alla somma lo indichiamo con -a e lo chiamiamo opposto.\nquindi a+(-a)=(-a)+a=0\nDEFINIZIONE (ANELLO UNITARIO)\nun anello (A,+,\\times) si dice unitario se esiste l’elemento neutro rispetto al prodotto.\nlo indichiamo con il simbolo 1 e lo chiamiamo unità.\nl’inverso di un elemento a\\in A, se esiste lo indichiamo con il simbolo a^-1.\nDEFINIZIONE (ELEMENTO NEUTRO +)\nsia (A,+, \\times) un anello.\nricordiamo che con 0 indichiamo l’elemento neutro della somma.\nPROPOSIZIONE\nsia (A,+, \\times) un anello.\na \\times 0=0 \\times a=0    \\forall a \\in A\nDIMOSTRAZIONE\na \\times 0=a \\times (0+0)=a \\times 0+a \\times 0\na \\times 0=a \\times 0 + a \\times 0 \\rightarrow\n\\ a \\times 0 +(-a \\times 0)=a \\times 0+(a \\times 0+(-a \\times 0)) \\rightarrow\n0=a \\times 0+0 \\rightarrow\n0=a \\times 0\nquindi si prova che 0 \\times a=0\nNOTAZIONE\nsia (A,+,\\times) un anello unitario\nsia a \\in A invertibile rispetto al prodotto.\ndenoteremo il suo inverso con a^-1, quindi\na \\times a^-1=a^-1 \\times a=1\nESEMPIO\n(Z,+,\\times) è un anello unitario.\ngli unici elementi invertibili rispetto al prodotto sono 1 e -1.\novviamente 1^-1=1 \\ \\ \\ (-1)^-1=-1\nOSSERVAZIONE\nsia (A,+, \\times) un anello unitario.\n0 è invertibile rispetto al prodotto?\n0 \\times x =1 \\rightarrow 0=1 può accadere solo se i due elementi neutri coincidono.\nma quando 0=1? a \\in A, se 0=1 , a \\times 0=0, a \\times 1=a \\rightarrow\n\\rightarrow a \\times 0=a \\rightarrow a=0 \\equiv a \\times 1=0 \\rightarrow a=0 \\rightarrow A=\\{0\\}\nquindi l’elemento neutro della somma è invertibile rispetto al prodotto solo se A contiene solo 0\nDEFINIZIONE (CORPO E CAMPO)\nun anello unitario si chiama corpo se tutti i suoi elementi non nulli sono invertibili rispetto al prodotto .\nun corpo si chiama campo se il prodotto è commutativo.\nESEMPI\n\n(1) CAMPO DEI NUMERI RAZIONALI\n(Q,+,\\times) è un campo\n(2) CAMPO DEI NUMERI REALI\n(R,+,\\times) è un campo\n(Z,+,\\times) è un anello unitario, ma non è un corpo.\n\nDEFINIZIONE (CAMPO DEI NUMERI COMPLESSI)\nsia C=R^2=R \\ X \\ R\ndefiniamo su C le due operazioni\n\nSOMMA\nsiano \\ (a,b),(c,d)\\in C\n(a,b)+(c,d)=(a+c,b+d)\nPRODOTTO\nsiano (a,b),(c,d)\\in C\n(a,b)\\times(c,d)=(ac-bd,ad+bc)\n\nESEMPIO\n(1,3)+(2,5)=(3,8)\n(1,3)\\times (2,5)=(1\\times 2-3\\times 5, 1\\times 5 +3 \\times 2=(-13,11)\n\n\n(1) (C,+) è un gruppo abeliano\ncon elemento neutro (0,0);\\ \\ \\forall (a,b)\\in C \\ \\ \\ -(a,b)=(-a,-b)\n\n\n(2) il prodotto è associativo (DA VERIFICARE)\n\n\n(3) valgono le proprietà distributive (DA VERIFICARE)\n\n\n(4) (1,0) è l’elemento neutro del prodotto\n(a,b)\\times(1,0)=(a\\times 1-b\\times 0, a\\times 0+b \\times 1)=(a,b)\n(1,0)\\times (a,b)=(1 \\times a - 0 \\times b, 1 \\times b+0 \\times a)=(a,b)\n\n\n(5) sia (a,b)\\in C  , (a,b)\\neq(0,0)\ndobbiamo risolvere l’equazione (a,b)\\times(x,y)=(1,0) \\rightarrow\n(ax-by+ay-bx)=(1,0)\n\\begin{cases} ax-by=1 \\\\ ay+bx=0  \\end{cases} \\rightarrow \\begin{cases} ax-by=1 \\\\ y=-\\frac{b}{a}x \\end{cases}\\rightarrow \\begin{cases}ax -b\\left( -\\frac{b}{a}x \\right)=1 \\\\ y=-\\frac{b}{a}x \\end{cases}\n\\begin{cases}ax -\\frac{b^2}{a}x=1 \\\\ y=-\\frac{b}{a}x\\end{cases}\\rightarrow \\begin{cases}a^2x-b^2x=a \\\\ y=-\\frac{b}{a}x  \\end{cases}\\rightarrow \\begin{cases} (a^2+b^2)x=1  \\\\ y=-\\frac{b}{a}x \\end{cases}\n\\begin{cases}x=\\frac{a}{(a^2+b^2)} \\\\y=-\\frac{b}{a}x  \\end{cases} \\rightarrow \\begin{cases}x=\\frac{a}{(a^2+b^2)}\\\\ y=-\\frac{b}{a} \\frac{a}{(a^2+b^2)} \\end{cases} \\rightarrow \\begin{cases}x= \\frac{a}{(a^2+b^2)} \\\\ y=-\\frac{b}{(a^2+b^2)}\\end{cases}\\rightarrow\nabbiamo dimostrato che  (a,b)^-1=\\left( \\frac{a}{(a^2+b^2)}, \\frac{b}{(a^2+b^2)} \\right)\n\n(6) la moltiplicazione è commutativa\n\nFORMA ALGEBRICA DI UN NUMERO COMPLESSO\nponiamo i=(0,1)\nPROPOSIZIONE\ni^2=-(1,0)\nDIMOSTRAZIONE\n(0,1)\\times(0,1)=(0-1,0+0)\\rightarrow\n(-1,0)\\rightarrow-(1,0)\nNOTAZIONE\ni numeri complessi possono essere rappresentati sul piano cartesiano:\nTransclude of Drawing-2025-10-08-21.09.51.excalidraw\nogni elemento dell’asse x corrisponde ad un numero reale:\n(a,0),a\\in A          R\\subset C\n\n\nDEFINIZIONE (a+bi)\na+bi si chiama forma algebrica del numero complesso\nDEFINIZIONE\nsia z\\in C, z=a+ib , si chama coniugato di Z il numero\nz^==a-ib\nsi chiama modulo di z il numero\n|z|=\\sqrt{ a^2+b^2 }\nsi chiama parte reale di z, il numero reale\nRe \\ z=a\nsi chiama parte immaginaria di z, il numero reale\nIm \\ z=b\nTransclude of Drawing-2025-10-08-22.08.48.excalidraw\nPROPOSIZIONE\nsia \\ z\\in C\nz \\in R \\leftarrow\\rightarrow z=z^=\nDIMOSTRAZIONE\nsia z=a+bi              z \\in R \\leftarrow\\rightarrow b=0\\leftarrow\\rightarrow z=a \\leftarrow\\rightarrow z^==a=z\nPROPOSIZIONE\nsia z\\in C\n\n(1)\nz\\times z^==|z|^2\n(2)\nse z \\neq 0 ,    z^-1=\\frac{z^=}{|z|^2}\n\nDIMOSTRAZIONE\n\n(1) sia z=a+bi ; z^==a-bi\nz\\times z^{=}=(a+bi)(a-bi)=a^2-abi+abi-b^2i^2=a^2+b^2i^2=a^2+b^2 \\rightarrow z\\times z^==|z|^2\n\nPROPOSIZIONE\nsia z \\in C,  z\\not\\in R\nz \\ e \\ z^=  sono soluzioni di una equazione di secondo grado a coefficienti reali\nDIMOSTRAZIONE\n(x-z)(x-z^=)=x^2-(z+z^=)x+z\\times z^=\nsia z=a+bi;\nx^{2}-(a+bi+a-bi)x+|z|^2\\rightarrow x^{2}-2(Re \\ z)+ |z|^2\nESEMPIO\nsia z=2+3i\nallora x^{2}-2(2)x+13\\rightarrow x^2-4x+13=0\nquindi z è soluzione dell’equazione, infatti:\n\\frac{\\Delta}{4}=4-13=-9 \\rightarrow \\sqrt{-9}=\\sqrt{(-1)\\times 9}=\\sqrt{-1}\\times \\sqrt{9}=3i\nx=\\frac{4\\pm 3i}{2} \\rightarrow x=2\\pm 3i\nMatrici\nsia (K,+,\\times) un campo (per esempio K=R).\nsiano m \\in Z, n \\in Z, \\ m&gt;0, n&gt;0\nsia I=\\{1,2,....,n\\} e J=\\{1,2,....,n\\}\nDEFINIZIONE  (MATRICE)\nsi chiama matrice con m righe ed n colonne ad elementi in K una funzione:\nI \\ X \\ J \\rightarrow K\nNOTAZIONE\nper rappresentare una matrice utilizziamo una tabella con m righe e n colonne\nM= \\begin{vmatrix} a_{11} \\ \\ a_{12} \\ \\ a_{13} \\ \\ a_{14} \\\\ a_{21}\\ \\ a_{22} \\ \\ a_{23} \\  \\  a_{24} \\\\ a_{31} \\ \\ a_{32} \\ \\ a_{33} \\ \\ a_{34}\\end{vmatrix}\ncon a_{ij}\nin questo caso m=3 \\ e \\ n=4\nNOTAZIONE\nponiamo K^{m,n}=insieme \\ di\\ tutte\\ le\\ matrici\\ con\\ m\\ righe\\ ed\\ n\\ colonne\\ ad\\ elementi\\ in\\ K\nESEMPIO\nsia A=\\begin{vmatrix} 2  &amp;  3 &amp;  4\\\\ 1  &amp;  5  &amp;  1    \\end{vmatrix} \\in R^{2,3}\nsia B=\\begin{vmatrix}1  &amp;  5  &amp;  2+3i  &amp;  7 \\\\ 7  &amp;  i  &amp;  4  &amp;  -2 \\end{vmatrix} \\in C^{2,4}\nNOTAZIONE\nsia M\\in K^{m,n}.\nponiamo ent_{ij}=l&#039;elemento \\ situato \\ sulla \\ riga \\ i \\ e \\ colonna \\ j  \\ della \\ matrice \\ M\nquindi ent_{ij}\\in K\nESEMPIO\nsia M= \\begin{vmatrix} 3 &amp; 8 &amp; 9 &amp; 6 &amp; 3 \\\\ 2 &amp; -1 &amp; 4 &amp; 1 &amp; 0 \\\\ 5 &amp; 6 &amp; 2 &amp; 1 &amp; 2               \\end{vmatrix} \\in R^{3,5}\nent_{2,4}(M)=1\nDEFINIZIONE (SCALARI)\ngli elementi del campo K si chiamano scalari\nDEFINIZIONE (Elementi)\nsiano A \\in K^{m,n}, B\\in K^{m,n}\ndefiniamo A+B \\in K^{m,n}\nent_{ij}(A+B)=ent_{ij}(A)+ent_{ij}(B)\nESEMPIO\nsiano A=\\begin{vmatrix}1 &amp; 0 &amp; 3 \\\\ 2 &amp; 4 &amp; 5  \\end{vmatrix} \\in R^{2,3}; B=\\begin{vmatrix}4 &amp; 6 &amp; -1 \\\\ 3 &amp; 1 &amp; 8\\end{vmatrix} \\in R^{2,3}\nA+B=\\begin{vmatrix}5 &amp; 6 &amp; 2 \\\\ 5 &amp; 5 &amp; 13 \\end{vmatrix}\nPROPOSIZIONE\n(K^{m,n},+) è un gruppo abeliano.\n\nl’operazione è binaria interna\nl’operazione è associativa \\rightarrow (A+B)+C=A+(B+C)\\ \\ \\ \\forall A,B,C \\in K^{m,n}\nesistenza dell’elemento neutro \\rightarrow A+0=0+A=A    \\forall A\\in K^{m,n}\nesistenza dell’inverso\nl’operazione è commutativa\n\nESEMPIO\nsia A=\\begin{vmatrix} 2 &amp; 3 &amp; -5 \\\\ 4 &amp; 2 &amp; 8 \\\\ 7 &amp; -6  &amp; -2 \\\\ 4 &amp; 1 &amp; 3     \\end{vmatrix} \\in R^{4,3};          sia -A=\\begin{vmatrix}-2 &amp; -3 &amp; 5 \\\\ -4 &amp; -2 &amp; -8 \\\\ -7 &amp; 6 &amp; 2 \\\\ -4 &amp; -1 &amp; -3     \\end{vmatrix} \\in R^{4,3}\nA+(-A)=0\nDEFINIZIONE (PRODOTTO DI UNO SCALARE PER UNA MATRICE)\nsia (K,+,\\times) un campo e siano a \\in K e M \\in K^{m,n}\nvogliamo definire una matrice a\\times M \\in K^{m,n} nel seguente modo\nent_{ij}(a\\times M)=a \\times ent_{ij}(M)\nESEMPIO\nsia M=\\begin{vmatrix}5 &amp; 1 &amp; -6 \\\\ -3 &amp; 2 &amp; 0 \\end{vmatrix} \\in R^{2,3} ,   a=3 \\in R\na \\times M=3 \\begin{vmatrix}5 &amp; 1 &amp; -6 \\\\ -3 &amp; 2 &amp; 0 \\end{vmatrix}= \\begin{vmatrix} 15 &amp; 3 &amp; -18 \\\\ -9 &amp; 6 &amp; 0   \\end{vmatrix} \\in R^{2,3}\nOSSERVAZIONE\nil prodotto di uno scalare per una matrice è una operazione del tipo:\nK\\ X \\ K^{m,n} \\rightarrow K^{m,n}\nPROPRIETA’\nsia (K,+, \\times) un campo.\n\n(a+b)\\times M=a\\times M + b\\times M              \\forall a,b \\in K, \\ \\ \\forall M \\in K^{m,n}\na \\times (M+N)=a\\times M +a\\times N            \\forall a \\in K, \\ \\ \\forall M,N \\in K^{m,n}\na\\times(b\\times M)=(a\\times b)\\times M                     \\forall a,b \\in K, \\ \\ \\forall M \\in K^{m,n}\n1 \\times M=M                                                 \\forall M \\in K^{m,n}\n\nDEFINIZIONE (PRODOTTO TRA MATRICI)\nsia (K,+,\\times) un campo e siano A \\in K^{m,n}, \\ \\ A \\in K^{m,n}\ndefiniamo il prodotto A\\times B \\in K^{m,n}\nent_ij(A\\times B)=\\sum_{r=1}^{n}ent_{ij}(A)\\times ent_{ij}(B)\nESEMPIO\nA=\\begin{vmatrix} 2 &amp; 3 &amp; 1 \\\\ 4 &amp; 1 &amp; 2    \\end{vmatrix} \\in R^{2,3};     B=\\begin{vmatrix} 1 &amp; -1 &amp; 2 &amp; 1 \\\\ 3 &amp; 2 &amp; 1 &amp; 0 \\\\ 2 &amp; 1 &amp; 4 &amp; 1    \\end{vmatrix} \\in R^{3,4}\nent_{ij}11=2\\times 1+3\\times 3+1\\times 2= 13\nent_{ij}21=4\\times 1+1\\times 3+2 \\times 2=11\nA\\times B= \\begin{vmatrix}13 &amp; 5 &amp; 11 &amp; 3 \\\\ 11 &amp; 0 &amp; 17 &amp; 6    \\end{vmatrix}\nPROPRIETA’\n\n(A\\times B)\\times C=A\\times(B\\times C)                       \\forall A\\in K^{m,n}, \\forall B\\in K^{m,p}, \\forall C \\in K^{n,q}\nA\\times(B + C)= A\\times B+A \\times C                  \\forall A\\in K^{m,n}, \\ \\ \\ \\forall B,C \\in K^{n,p}\n(a\\times M)\\times N=a\\times (M\\times N)=M\\times(a\\times N)     \\forall a \\in K,\\ \\ \\forall M \\in K^{m,n}, \\ \\forall N \\in K^{n,p}\n\nOSSERVAZIONE\nnon sempre vale la proprietà commutativa.\nsia A=\\begin{vmatrix}1 &amp; 0 \\\\ 0 &amp; 0     \\end{vmatrix} \\in K^{2,2} ,   B=\\begin{vmatrix}0 &amp; 1 \\\\0 &amp; 0     \\end{vmatrix} \\in K^{2,2}\nA \\times B= \\begin{vmatrix} 0 &amp; 1 \\\\ 0 &amp; 0     \\end{vmatrix}= B\nB \\times A=\\begin{vmatrix} 0 &amp; 0 \\\\ 0 &amp; 0    \\end{vmatrix}=0\nA\\times B \\neq B\\times A\nESERCIZIO\ndefinire l’insieme delle matrici R^{2,2} che commutano con A=\\begin{vmatrix} 1 &amp; 0 \\\\ 0 &amp; 0    \\end{vmatrix}.\nX=\\begin{vmatrix} x &amp; y \\\\ z &amp; t    \\end{vmatrix}\nA \\times X= \\begin{vmatrix} x  &amp; y \\\\ 0 &amp; 0    \\end{vmatrix}\nX\\times A= \\begin{vmatrix}x &amp; 0 \\\\ z &amp;  0     \\end{vmatrix}\nSOLUZIONE=\\begin{cases}x=x\\\\ y=0 \\\\ z=0 \\\\ 0(t)=0 \\end{cases} \\rightarrow X_{sol}=x\\begin{vmatrix}x &amp; 0 \\\\ 0 &amp;  0     \\end{vmatrix} +  t\\begin{vmatrix}0 &amp; 0 \\\\0  &amp; t   \\end{vmatrix}    \\forall x,t \\in R \\rightarrow LA SOLUZIONE è X_{sol}=\\begin{vmatrix}x &amp; 0 \\\\ 0 &amp; t \\end{vmatrix}\nDEFINIZIONE (MATRICE QUADRATA)\nsia A \\in K^{m,n}.\ndiremo che A è una matrice quadrata se m=n.\nOSSERVAZIONE\nil prodotto di matrici in K^{n,n} è una operazione binaria interna\nPROPOSIZIONE\n(K^{n,n},+,\\times) è un anello unitario \\forall n  \\geq 1\nDIMOSTRAZIONE\ndobbiamo dimostrare che il prodotto ha l’elemento neutro.\nl’elemento neutro per il prodotto è la matrice:\nI=\\begin{vmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1     \\end{vmatrix}\ndobbiamo dimostrare che AI=IA=A\nent_{ij}(AI) =\\sum_{r=1}^{n}ent_{i,r}(A)\\times ent_{r,j}(I) \\rightarrow ent_{i,j}(A)\\times ent_{j,j} (I) = ent_{i,j}(A)\nAI=A\nent_{ij}=\\sum_{r=1}^nent_{i,r}(I)\\times ent_{r,j}(A) \\rightarrow ent_{i,i}(I)\\times ent_{i,j}(A) =ent_{i,j}(A)\nIA=A\nQUINDI AI=IA=A\nESEMPIO\nse n=2     I=\\begin{vmatrix}1 &amp; 0 \\\\ 0 &amp; 1     \\end{vmatrix}\nse n= 3     I=\\begin{vmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0  \\\\ 0 &amp; 0 &amp; 1     \\end{vmatrix}\nDOMANDA\nin K^{n,n} quali matrici sono invertibili rispetto al prodotto?\nOSSERVAZIONE\nnon tutte le matrici non nulle sono invertibili.\nsia A=\\begin{vmatrix}1 &amp; 0 \\\\ 0 &amp; 0     \\end{vmatrix} .     A non è invertibile.\ninfatti sia B=\\begin{vmatrix}0 &amp; 1 \\\\ 0 &amp; 0     \\end{vmatrix} sappiamo che\nA\\times B=B    e     B\\times A =0\nse A fosse invertibile esisterebbe l’inversa A^-1\n(B\\times A)\\times A^{-1}=0\\times A^-1 \\rightarrow B\\times (A\\times A^-1)=0 \\rightarrow B\\times I =0\n\\rightarrow B=0 (FALSO)\nDEFINIZIONE (DIAGONALI)\nsia A\\in K^{n,n}\n\nDEFINIZIONE (TRIANGOLARE SUP e INF)\nsia A\\in K^{n,n}.\nA si dice:\n\nTRIANGOLARE SUPERIORE\nse tutti gli elementi al di sotto della diagonale principale sono nulli, ovvero\nent_{ij}(A)=0 \\ \\ \\  \\forall i&gt;j\nTRIANGOLARE INFERIORE\nse tutti gli elementi al di sopra della diagonale principale sono nulli, ovvero\nent_{ij}(A)=0 \\ \\ \\ \\forall i&lt;j\n\n\nDEFINIZIONE (MATRICE DIAGONALE)\nsia A\\in K^{n,n}.\nA si dice diagonale se è sia triangolare inferiore che triangolare superiore.\n\nDEFINIZIONE (MATRICE SCALARE)\nsia A \\in K^{n,n}.\nA si dice Matrice Scalare se esiste a\\in K, tale che A=a\\times I\nS=a\\times I=a\\times \\begin{vmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{vmatrix}=\\begin{vmatrix}a &amp; 0 &amp; 0 \\\\ 0 &amp; a &amp; 0 \\\\ 0 &amp; 0 &amp; a\\end{vmatrix}\nOSSERVAZIONE\nMATRICE SCALARE \\rightarrow MATRICE DIAGONALE \\rightarrow MATRICE TRIANGOLARE\nDEFINIZIONE (MATRICE TRASPOSTA)\nsia A \\in K^{n,n}.\nsi chiama matrice trasposta di A la matrice A^{t}\\in K^{n,n} tale che:\nent_{ij}(A^t)=ent_{ji}(A)\nESEMPIO\nsia A=\\begin{vmatrix}a &amp; b &amp; c \\\\ d &amp; e &amp; f  \\end{vmatrix}\\in K^{2,3} \\rightarrow A^{t}=\\begin{vmatrix}a &amp; d \\\\ b &amp; e \\\\ c &amp; f     \\end{vmatrix}\\in K^{3,2}\nPROPRIETA’\n\n(1)\n(A+B)^{t}=A^{t}+B^{t} ,   \\forall A,B \\in K^{m,n}\n(2)\n(a\\times M)^{t}=a\\times M^{t},    \\forall a\\in K, \\forall M\\in K^{m,n}\n(3)\n(A\\times B)^{t}=B^{t}\\times A^{t} ,   \\forall A\\in K^{m,n}, \\forall B\\in K^{n,p}\n(4)\n(A^{t})^{t}=A,   \\forall A\\in K^{m,n}\n\nDEFINIZIONE (TRACCIA DI UNA MATRICE QUADRATA)\nsia A\\in K^{n,n}\nsi chiama Traccia di A la somma degli elementi della diagonale principale, ovvero\ntr(A)=\\sum_{r=1}^{n}ent_{rr}(A)\n\nPROPRIETA’\n\n(1)\ntr(A+B)=tr(A)+tr(B),   \\forall A,B \\in K^{n,n}\n(2)\ntr(a\\times M)=a\\times tr(M),    \\forall a\\in K, \\forall M\\in K^{n,n}\n(3)\ntr(A\\times B)=tr(B\\times A),     \\forall A,B \\in K^{n,n}\n\nESEMPIO\nsia A=\\begin{vmatrix}a &amp; b \\\\ c &amp; d \\end{vmatrix} ,  B=\\begin{vmatrix}e &amp; f \\\\ g &amp; h  \\end{vmatrix}\nA\\times B=\\begin{vmatrix}ae+bg &amp; af+bh \\\\ ce+dg &amp; cf+dh\\end{vmatrix}     tr(AB)=ae+bg+cf+dh;\nB\\times A=\\begin{vmatrix}ea+fc &amp; eb+fd \\\\ ga+hc &amp; gb+hd\\end{vmatrix}     tr(BA)=ea+fc+gb+hd\ntr(AB)=tr(BA);\nDEFINIZIONE (FUNZIONI)\nsiano A e B insiemi.\nsia A\\rightarrow B una funzione.\nf si dice iniettiva se f(a_1)=f(a_2)\\implies a_{1}=a_{2}\nf si dice suriettiva se Im f=B, ovvero \\forall b\\in B, \\ \\ \\exists a\\in A  tale che f(a)=b.\nf si dice biettiva se è sia iniettiva che suriettiva.\nESEMPIO\n\nDEFINIZIONE (PERMUTAZIONE)\nsia A un insieme.\nsi chiama permutazione su A una funzione biettiva A\\rightarrow A.\nse A=\\{1,2,3,....,n\\}, indicheremo con S_n l’insieme di tutte le permutazioni su A.\nESEMPIO\nsia n=3,  A=\\{1,2,3\\}\ngli elementi di S_3 sono:\n\nin generale in S_n ci sono n! permutazioni\nn!=\\prod_{r=1}^{n}r\nESERCIZIO PER CASA\nscrivere tutti gli elementi di S_4\ntutti gli elementi di S_4 sono 4!=24\nESEMPIO\nsia (5,1,3,6,4,2)\\in S_6\n\n\nper arrivare alla permutazione fondamentale, abbiamo eseguito 4 scambi, ed essendo 4 un numero pari, diremo che questa è una permutazione di classe pari.\nDEFINIZIONE (CLASSE DI UNA PERMUTAZIONE)\nsia \\sigma \\in S_n .\ndiremo che \\sigma è di classe pari se occorrono un numero pari di scambi per trasformarla nella permutazione fondamentale, altrimenti diremo che \\sigma è di classe dispari.\nDEFINIZIONE (SEGNO DI UNA PERMUTAZIONE)\nsia \\sigma \\in S_n.\nchiamiamo Segno di una permutazione la funzione:\n\\begin{cases}1 \\ \\ s e \\ \\sigma \\ e&#039;\\ pari \\\\ -1 \\ se \\ \\sigma \\ e&#039;\\ dispari\\end{cases}\nESERCIZIO\ncalcolare i segni delle permutazioni in S_3\nS_{3} = \\{(1,2,3),(1,3,2),(3,2,1),(2,1,3),(2,3,1),(3,1,2)\\}\nS_3=\\{+,-,-,-,+,+\\}\nDEFINIZIONE (PRODOTTO DEDOTTO)\nsia A\\in K^{n,n}.      sia \\sigma \\in S_n .\nsi chiama prodotto dedotto associato alla permutazione \\sigma, il prodotto\nent_{1 \\sigma(1)} \\times ent_{2 \\sigma (2)} \\times ent_{n \\sigma (n)}\nESEMPIO\nsia A=\\begin{vmatrix}a_{11} &amp; a_{12} &amp; *a_{13}* &amp; a_{14} \\\\ *a_{21}* &amp; a_{22} &amp; a_{23} &amp; a_{24}  \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; *a_{34}* \\\\ a_{41} &amp; *a_{42}* &amp; a_{43}&amp; a_{44} \\end{vmatrix} \\in K^{4,4}\nsia \\sigma =(3,1,4,2) \\in S_{4} \\implies p\\sigma =a_{13}\\times a_{21}\\times a_{34}\\times a_{42}\nDEFINIZIONE (DETERMINANTE DI A)\nsia A\\in K^{n,n}.\nsi chiama determinante di A lo scalare\ndet A=\\sum_{\\sigma \\in S_{n}}sgn(\\sigma)\\ p\\sigma\nESEMPIO\nn=1         \\ \\ A=(a_{11}) \\ \\ \\ \\implies det\\ A=a_{11}\nn=2         A=\\begin{vmatrix}a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{vmatrix};    S_{2}=\\{(1,2),(2,1)\\};\nquindi det A=p_{1,2}-p_{2,1}=a_{11}\\times a_{22}-a_{12}\\times a_{21}\nESEMPIO\nA=\\begin{vmatrix}2 &amp; 3 \\\\ 4 &amp; 5     \\end{vmatrix};              det\\ A=2\\times 5-3\\times 4=10-12=-2\nn=3           A=\\begin{vmatrix}a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{vmatrix}\\in K^{3,3}\nS_{3}=\\{(1,2,3),(1,3,2),(3,2,1),(2,1,3),(2,3,1),(3,1,2)\\}\ndetA=p_{1,2,3}-p_{1,3,2}-p_{3,2,1}-p_{2,1,3}+p_{2,3,1}+p_{3,1,2}\n=a_{11}a_{22}a_{33}-a_{11}a_{23}a_{32}-a_{13}a_{22}a_{31}-a_{12}a_{21}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}\n\nESEMPIO\nsia A=\\begin{vmatrix}2 &amp; 1 &amp; 1 \\\\ 1 &amp; 3 &amp; 5 \\\\ 3 &amp; 2 &amp; 1 \\end{vmatrix} S_{3}=\\{(1,2,3),(1,3,2),(3,2,1),(2,1,3),(2,3,1),(3,1,2)\\}\npermutazioni pari=(1,2,3),(2,3,1)(3,1,2)\npermutazioni dispari=(1,3,2),(3,2,1),(2,1,3)\nDet A=2\\times3\\times1+1\\times 5\\times 3+1\\times 1\\times 2-2\\times5\\times 2-1\\times 3\\times 3-1\\times1\\times1\nDetA=6+15+2-20-9=23-29=-7\nPROPRIETA’ (DETERMINANTE)\nsia A\\in K^{n,n}\\implies A=(C_{1},\\dots,C_{n}) dove C_{1},\\dots, C_{n} sono le n colonne della matrice.\nquindi C_{i}\\in K^{n,1}, \\ per\\ 1\\leq i\\leq n .\nper esempio consideriamo la matrice\nA=\\begin{vmatrix}\\frac {1}{4} &amp;2 \\frac{3}{1}+3\\frac{2}{5}\\end{vmatrix}\nC_{2}=2 \\frac{3}{1}+3\\frac{2}{5}=\\frac {6}{2}+\\frac{6}{15}=\\frac{12}{17}\nDEFINIZIONE (COMBINAZIONE LINEARE)\nsiano a,b \\in K , M,N \\in K^{m,n}\nl’operazione a\\times M+b\\times N si chiama COMBINAZIONE LINEARE\n\nDEFINIZIONE (SISTEMA DI CRAMER)\nun sistema lineare AX=B si dice sistema di cramer se\n\nA è una matrice quadrata\n\\det A\\neq{0}\n\nTEOREMA DI CRAMER\nun sistema lineare di cramer ha sempre una ed una sola soluzione\nDIMOSTRAZIONE\nsia AX=B un sistema di cramer \\implies A è quadrata e \\det A \\neq 0 \\implies A è invertibile.\nA^-1(AX)=A^-1B  \\implies (AA^-1)X=A^-1B \\implies IX=A^-1B \\implies X=A^-1B\nproviamo che X=A^-1B è una soluzione:\nA(A^-1B)=(AA^{-1})B=IB=B\\implies X=A^-1B\nESERCIZIO\n\\begin{cases}2x+5y=11 \\\\ 3x+4y =13   \\end{cases}\nSOLUZIONE\nA=\\begin{pmatrix}2 &amp; 5 \\\\ 3 &amp; 4\\end{pmatrix}    X=\\begin{pmatrix}x \\\\ y\\end{pmatrix}\nB=\\begin{pmatrix}11  \\\\ 13\\end{pmatrix}\n\\det A=8-15=-7 \\implies \\text{il sistema è di cramer}\n\\begin{pmatrix}2 &amp; 5 \\\\ 3 &amp; 4\\end{pmatrix}\\cdot \\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}11 \\\\ 13\\end{pmatrix}\nA_{AGG}=A^T_{cof}=\\begin{pmatrix}4 &amp; -3 \\\\ -5 &amp; 2\\end{pmatrix}=\\begin{pmatrix}4 &amp; -5 \\\\ -3 &amp; 2\\end{pmatrix}\nA^{-1}=-\\frac{1}{7} \\cdot \\begin{pmatrix}4 &amp; -5 \\\\ -3 &amp; 2\\end{pmatrix}\n\\begin{pmatrix}4 &amp; -5 \\\\ -3 &amp; 2\\end{pmatrix}\\cdot \\begin{pmatrix}2 &amp; 5 \\\\ 3 &amp; 4\\end{pmatrix}\\cdot\\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}4 &amp; -5 \\\\ -3 &amp; 2\\end{pmatrix}\\cdot \\begin{pmatrix}11 \\\\ 13\\end{pmatrix} \\implies\n\\begin{pmatrix}-7 &amp; 0 \\\\ 0 &amp; -7\\end{pmatrix}\\cdot \\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}4 &amp; -5 \\\\ -3 &amp; 2\\end{pmatrix}\\cdot \\begin{pmatrix}11 \\\\ 13\\end{pmatrix}\n\\begin{pmatrix}-7 &amp; 0 \\\\ 0 &amp; -7\\end{pmatrix}\\cdot \\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}44-65 \\\\ -33+26\\end{pmatrix}\n\\begin{pmatrix}-7 &amp; 0 \\\\ 0 &amp; -7\\end{pmatrix}\\cdot \\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}-21 \\\\ -7\\end{pmatrix}\\implies \\begin{cases}-7x=-21 \\\\ -7y=-7\\end{cases}\n\\implies \\begin{cases}x=\\frac{-21}{7}=3 \\\\ y=\\frac{-7}{-7}=1\\end{cases}\nLA SOLUZIONE E’ X=\\begin{pmatrix}3 \\\\ 1\\end{pmatrix}\nREGOLA DI CRAMER\nsia AX=B un sistema di cramer.\nA\\in K^{n,n}\nscriviamo A per colonne, A=(C_{1}\\dots C_{n}) .\nponiamo A_{(i)}=(C_{1}\\dots C_{i-1} \\ B \\ C_{i+1}\\dots C_{n})\n(in A sostituiamo la colonna i-esima con la colonna B)\nallora la soluzione del sistema è S=\\begin{pmatrix}s_{1} \\\\ . \\\\ . \\\\s_{n} \\end{pmatrix}\n$$$s_{i}=\\frac{\\det A_{i}}{\\det A}$$\nESERCIZIO\n\\begin{cases}x+2y+3z=1 \\\\ 2x+3y+z=3 \\\\ x+y+4z=2\\end{cases}            x \\begin{pmatrix}1 \\\\ 2 \\\\ 1\\end{pmatrix}+y \\begin{pmatrix}2 \\\\ 3 \\\\ 1\\end{pmatrix}+z \\begin{pmatrix}3 \\\\ 1 \\\\ 4\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 3 \\\\ 2\\end{pmatrix}\nSOLUZIONE\nA=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 2 &amp; 3 &amp; 1 \\\\ 1 &amp; 1 &amp; 4\\end{pmatrix};     \\det A=12+2+6-9-1-16=-6 \\implies il sistema è di cramer\nx= \\frac{\\det A_{(1)}}{\\det A}=-\\frac{1}{6}\\det \\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 3 &amp; 3 &amp; 1 \\\\ 2 &amp; 1 &amp; 4\\end{pmatrix}=-\\frac{1}{6}(12+4+9-18-1-24)=\\frac{-18}{-6}=3\ny=\\frac{\\det A_{2}}{\\det A}=-\\frac{1}{6}\\det \\begin{pmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 3 &amp; 3 \\\\ 1 &amp; 1 &amp; 2\\end{pmatrix}=-\\frac{1}{6}(6+6+2-3-3-8)=-\\frac{1}{6}(25-19)=-1\nz=\\frac{\\det A_{3}}{\\det A}=\\frac{1}{6}\\det \\begin{pmatrix}1 &amp; 2 &amp; 1 \\\\ 2 &amp; 3 &amp; 3  \\\\ 1 &amp; 1 &amp; 2\\end{pmatrix}=-\\frac{1}{6}\\det \\begin{pmatrix}6+6+2-3-3-3-8\\end{pmatrix}=-\\frac{1}{6}(14-14)=0\n\\implies la  soluzione del sistema è X=\\begin{pmatrix}3 \\\\ -1 \\\\ 0\\end{pmatrix}\nESERCIZIO\n\\begin{cases}hx+hy=2 \\\\ 3x+hy=5\\end{cases}          h\\in R; quando è un sistema di cramer\nSOLUZIONE\n\\begin{pmatrix}h &amp; h \\\\ 3 &amp; h\\end{pmatrix}\\begin{pmatrix}x \\\\ y\\end{pmatrix}=\\begin{pmatrix}2 \\\\ 5\\end{pmatrix} ;\n\\det A=h^2-3h=h(h-3)\n\\det A=0 \\iff h(h-3)\\iff h=0 \\ \\text{oppure} \\ h=3\nper ogni h=0 e h=3 il sistema non è di cramer\nsupponiamo h\\neq 0 e h\\neq 3.\nx=\\frac{1}{h(h-3)}\\det \\begin{pmatrix}2 &amp; h \\\\ 5  &amp; h\\end{pmatrix}=\\frac{2h-5h}{h(h-3)}=\\frac{-3h}{h(h-3)}=\\frac{3}{3-h}\ny=\\frac{1}{h(h-3)}\\det \\begin{pmatrix}h &amp; 2 \\\\ 3 &amp; 5\\end{pmatrix}=\\frac{5h-6}{h(h-3)}\n\\implies la soluzione del sistema è X=\\begin{pmatrix}\\frac{3}{3-h} \\\\ \\frac{5h-6}{h(h-3)}\\end{pmatrix}\nRANGO DI UNA MATRICE\nDEFINIZIONE (SOTTOMATRICE)\nsia  A\\in K^{m,n}.        sia I\\subseteq \\{1,\\dots,m\\}  e  J\\subseteq\\{1,\\dots,n\\}\nla matrice A_{I,J}=(a_{i,j}|\\ i\\in I\\wedge j\\in J) si chiama sottomatrice di A individuata da I e J\nESEMPIO\nsia A=\\begin{pmatrix}a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} &amp; a_{15} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24}  &amp; a_{25}  \\\\ a_{31}  &amp; a_{32} &amp; a_{33} &amp; a_{34} &amp; a_{35}\\end{pmatrix}\\in K^{3,5}\nprendiamo I=\\{1,3\\},\\ J=\\{2,3,5\\}\\implies A_{I,J}=\\begin{pmatrix}a_{12} &amp; a_{13 }  &amp; a_{15}  \\\\ a_{32} &amp; a_{33} &amp; a_{35}\\end{pmatrix}\\in K^{2,3}\nA_{I,J} è una sottomatrice di A.\nDEFINIZIONE (minore di ordine r)\nsia A\\in K^{m,n}    si chiama minore di ordine r di A, il determinante di una sottomatrice quadrata di A, di ordine r (ovvero con r righe e r colonne)\nESEMPIO\nsia A=\\begin{pmatrix}1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ 3 &amp; 8 &amp; 6 &amp; 1 &amp; 3 \\\\ 5 &amp; 1 &amp; 9 &amp; 2 &amp; 4\\end{pmatrix}\\in R^{3,5}\nI=\\{1,2\\},  J=\\{3,5\\} \\implies A_{I,J}=\\begin{pmatrix}3 &amp; 5 \\\\ 6 &amp; 3\\end{pmatrix};\nM=\\det A_{I,J}\nDEFINIZIONE (rango di una matrice)\nsia A\\in K^{m,n}.\nsi chiama RANGO DI A l’ordine massimo di un minore (M) non nullo di A.\nNOTAZIONE\nindicheremo il rango di A con rk (A)\nESEMPIO\nsia A=\\begin{pmatrix}2 &amp; 3 &amp; 4 &amp; 2 &amp; 1 \\\\ 5 &amp; 1 &amp; 3 &amp; 1 &amp; 4 \\\\ 7 &amp; 4 &amp; 7 &amp; 3 &amp; 5\\end{pmatrix}\\in K^{3,5}        (notiamo che R_{3}=R_{1}+R_{2})\ncalcoliamo il rango di A\nM=\\det \\begin{pmatrix}2 &amp; 3 \\\\ 5 &amp; 1\\end{pmatrix}=2-15=-13 \\implies rk(A)\\geq 2\ninoltre essendo R_{3}=R_{1}+R_{2}, tutti i minore di ordine 3 sono nulli di A sono nulli.\nrk(A)=2\nDEFINIZIONE (matrice ridotta per righe)\nsia A\\in K^{m,n} , A si dice ridotta per righe se in ogni riga non nulla esiste un elemento non nullo (detto pivot) al di sotto del quale vi sono solo zeri.\nESEMPIO\nsia A=\\begin{pmatrix}4 &amp; 7 &amp; 5 &amp; *9* &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 3 &amp; *4* &amp; 2 &amp; 0 &amp; 5 &amp; 1 \\\\ 6 &amp; 0 &amp; 9 &amp; 0 &amp; 2 &amp; *3* \\\\ *7* &amp; 0 &amp; 2 &amp; 0 &amp; 5 &amp; 0\\end{pmatrix}\nA è ridotta per righe.\nconsideriamo il minore individuato dai pivot:\nM=\\det A_{\\{1,3,4,5\\}\\{1,2,4,6\\}}=\\det \\begin{pmatrix}4 &amp; 7 &amp; 9 &amp; 3 \\\\ 3 &amp; 4 &amp; 0 &amp; 1 \\\\ 6 &amp; 0 &amp; 0 &amp; 3 \\\\ 7 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix}=\\pm \\det \\begin{pmatrix}9 &amp; 7 &amp; 3 &amp; 4 \\\\ 0 &amp; 4 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 3 &amp; 6 \\\\ 0 &amp; 0 &amp; 0 &amp; 7\\end{pmatrix}\\neq0 (abbiamo spostato le colonne)\nrk(A)=4\nPROPOSIZIONE\nil rango di una matrice ridotta per righe è uguale al numero di righe non nulle.\nMETODO DI RIDUZIONE\nogni matrice si può trasformare in una matrice ridotta per righe, senza alterare il rango.\nuna trasformazione che non altera il rango della matrice è:\nR_{i}=R_{i}+a\\cdot R_{j}, \\ \\text{con} \\ j\\neq i,  \\ a\\in K\nESERCIZIO\nA=\\begin{pmatrix}2 &amp; 3 &amp; 4 &amp; 2 &amp; 1 \\\\ 5 &amp; 1 &amp; 3 &amp; 1 &amp; 4 \\\\ 7 &amp; 4 &amp; 7 &amp; 3 &amp; 5\\end{pmatrix}\\in K^{3,5}\nA  (R_{2}\\equiv R_{2}-4R_{1})(R_{3}\\equiv R_{3}-5R_{1})\\implies \\begin{pmatrix}2 &amp;  3 &amp; 4 &amp; 2 &amp; 1  \\\\ -3 &amp; -11 &amp; -13 &amp; -7 &amp; 0 \\\\ -3 &amp; -11 &amp; -13 &amp; -7 &amp; 0\\end{pmatrix}\n(R_{3}\\equiv R_{3}-R_{2})\\implies \\begin{pmatrix}2 &amp; 3 &amp; 4 &amp; 2 &amp; 1 \\\\ -3 &amp; -11 &amp; -13 &amp; -7 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix} \\implies rk(A)=rk \\begin{pmatrix}2 &amp; 3 &amp; 4 &amp; 2 &amp; *1* \\\\ -3 &amp; -11 &amp; -13 &amp; *-7* &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{pmatrix} rk(A)=2\nNOTAZIONE\nsia AX=B un sistema lineare.\nsia (A|B) la matrice ottenuta da A affiancando la colonna B\nquindi se A\\in K^{m,n}\\implies(A|B)\\in K^{m,n+1}\nTEOREMA DI ROUCHE-CAPELLI\nun sistema lineare AX=B è risolubile \\iff rk(A)=rk(A|B)\nESERCIZIO\n\\begin{cases}2x+3y+5y=1 \\\\ x+4y+2z=3\\end{cases}\nSOLUZIONE\n\\begin{pmatrix}2 &amp; 3 &amp; 5 &amp; 1 \\\\ 1 &amp; 4 &amp; 2 &amp; 3\\end{pmatrix};       \\det \\begin{pmatrix}2 &amp; 3 \\\\ 1 &amp; 4\\end{pmatrix}=8-3=5\\neq 0\n\\implies rk\\begin{pmatrix}2 &amp; 3 &amp; 5 \\\\ 1 &amp; 4 &amp; 2\\end{pmatrix}=rk \\begin{pmatrix}2 &amp; 3 &amp; 5 &amp; 1 \\\\ 1 &amp; 4 &amp; 2 &amp; 3\\end{pmatrix}=2 \\implies il sistema è risolubile\nisoliamo z dato che è la variabile libera\n\\begin{cases}2x+3y=1-5z \\\\ x+4y=3-2z\\end{cases}\nx=\\frac{1}{5}\\det \\begin{pmatrix}1-5z &amp; 3 \\\\ 3-2z &amp; 4\\end{pmatrix}=\\frac{1}{5}[4\\cdot(1-5z)-3(3-2z)]=\\frac{1}{5}(4-20z-9+6z)=\\frac{-5-14z}{5}\ny=\\frac{1}{5}\\det \\begin{pmatrix}2 &amp; 1-5z \\\\ 1 &amp; 3-2z\\end{pmatrix}=\\frac{1}{5}[2(3-2z)-(1-5z)]=\\frac{1}{5}(6-4z-1+5z)=\\frac{5+z}{5}\nl’insieme delle soluzioni è y=\\{\\frac{-5-14z}{5},\\frac{5+z}{5},z|z\\in R\\}\nil sistema ha \\infty^1 soluzioni\nNOTAZIONE\nsia AX=B un sistema lineare risolubile.\nsia r=rk(A)=rk(A|B).\nsia A\\in K^{m,n} .\nil numero di variabili libere è n-r e diremo che il sistema ha \\infty^{n-r} soluzioni\nnel caso n=r il sistema ha una ed una sola soluzione\nESERCIZIO\n\\begin{cases}2x+y=5 \\\\ x+y=3 \\\\ 3x-y=h\\end{cases} ;               h\\in R\nSOLUZIONE\n(A|B)=\\begin{pmatrix}2 &amp;  1 &amp; 5  \\\\ 1 &amp; 1 &amp; 3 \\\\ 3 &amp; -1 &amp; h\\end{pmatrix};    \\det (A|B)=2h+9-5-15+6-h=h-5\n\\det (A|B)=0\\iff h-5=0\\iff h=5\nper h=5\nA|B=\\begin{pmatrix}2 &amp; 1 &amp; 5 \\\\ 1 &amp; 1 &amp; 3 \\\\ 3 &amp; -1 &amp; 5\\end{pmatrix}\\implies rk(A)=rk(A|B)=2\\implies il sistema è risolubile.\nn=2, r=2 \\implies n-r=0 \\implies il sistema ha una ed una sola soluzione\n\\begin{cases}2x+y=5 \\\\ x+y=3\\end{cases}\\implies \\begin{pmatrix}2 &amp; 1 &amp; 5 \\\\ 1 &amp; 1 &amp; 3\\end{pmatrix}(R_{2}\\equiv R_{2}-R_{1})\\implies \\begin{pmatrix}2 &amp; 1 &amp; 5 \\\\ -1 &amp; 0 &amp; -2\\end{pmatrix}\\implies \\begin{pmatrix}2 &amp; 1 &amp; 5 \\\\ 1 &amp; 0 &amp; 2\\end{pmatrix}\n\\begin{cases}2x+y=5 \\\\ x=2\\end{cases}\\implies \\begin{cases}x=2 \\\\ y=1\\end{cases}            X=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}\nSpazi Vettoriali\nricordiamo che (K^{m,n},+) è un gruppo abeliano.\ne ricordiamo che anche il prodotto di uno scalare per una matrice\nK\\times K^{m,n}\\rightarrow K^{m,n}\nDEFINIZIONE (K-spazio vettoriale)\nsia (K,+,\\cdot) un campo i cui elementi si chiamano scalari.\nsia (V,+) un gruppo abeliano i cui elementi sono vettori.\ndiremo che V è un K-spazio vettoriale se è definito un prodotto tra uno scalare e un vettore il cui risultato è un vettore, ovvero K\\times V \\rightarrow V che gode delle seguenti proprietà:\n\n(a+b)\\cdot v=a\\cdot v+b\\cdot v,       \\forall a,b\\in K,\\  \\forall v\\in V\na(u+v)=a\\cdot u+a\\cdot v,        \\forall a\\in K,   \\ \\ \\forall u,v\\in V\na(b\\cdot v)=(a\\cdot b)\\cdot v,               \\forall a,b\\in K, \\ \\ \\forall v\\in V\n1\\cdot v=v,                               \\forall v\\in V            (1 è l’unità in K)\n\nESEMPIO\nV=K^{m,n} è uno spazio vettoriale su K\nin particolare V=K^{n}=K^{1,n} è un K-spazio vettoriale\nNOTAZIONE\nsia V un K-spazio vettoriale.\n(K,+,\\cdot) è un campo, gli elementi neutri di K sono 0 e 1.\n(V,+) è un gruppo abeliano, l’elemento neutro di V si indica \\theta e si chiama vettore nullo\nLEGGE DI ANNULLAMENTO DEL PRODOTTO\nsia V un K-spazio vettoriale.\nsiano a\\in K, \\ v\\in V .\nDIMOSTRAZIONE\nsupponiamo a=0\na\\cdot v=\\theta=(0+0)\\cdot v=0\\cdot v+0\\cdot v \\implies 0\\cdot v=0\\cdot v+0\\cdot v\\implies \\theta=0\\cdot v\nsupponiamo v=\\theta\na\\cdot v=a\\cdot O_{-}=a(O_{-}+O_{-})=a\\cdot O_{-}=a\\cdot O_{-}+a\\cdot O_{-}=O_{-}=a\\cdot O_{-}\nsupponiamo a\\cdot v=\\theta\nse a=0 abbiamo finito, quindi possiamo supporre a\\neq 0 \\implies a è invertibile rispetto al prodotto in K.\nsia a^-1 il suo inverso:\na\\cdot v=\\theta \\implies a^{-1}(a\\cdot v)=a^{-1}\\cdot \\theta\\implies (a^{-1}\\cdot a)\\cdot v=\\theta\\implies 1\\cdot v=\\theta \\implies v=\\theta\nDEFINIZIONE\nsia V un K-spazio vettoriale.\nsia S \\subseteq V, diremo che S è un sottospazio di V se S è a sua volta un K-spazio vettoriale rispetto alle stesse operazioni definite su V\n\nPROPOSIZIONE\nsia V un K-spazio vettoriale, sia S\\subseteq V.\nS è un sottospazio di V se e solo se valgono le seguenti proprietà:\n\n\\theta \\in S\n\\forall a,b\\in K, \\ \\forall u,v\\in S \\implies a\\cdot u+b\\cdot v\\in S            chiusura rispetto alla combinazione lineare\n\nESEMPIO\nsia V=R^2,            sia S=\\{(x,y)\\in R^{2}| \\ x+2y=1\\}\\subseteq R^{2}\nS non è un sottospazio, infatti \\theta = (0,0)\\not\\in S.\nESEMPIO\nsia V=R^2;  sia S=\\{(x,y)\\in R^2|x^2-y^2=0\\}\\subseteq R^2;\n\\vec{u}=(1,1)\\in S,  \\vec{v}=(1,-1)\\in S \\implies \\vec{v}+\\vec{u}=(2,0)\\not\\in S\n\\implies S non è un sottospazio, dato che la somma non è interna ad S.\nPROPOSIZIONE\nsia A\\in K^{m,n}.  sia S=\\{X\\in K^n|AX=\\theta\\}\\subseteq K^n.\nS è un sottospazio di K^n.\nDIMOSTRAZIONE\n\n\\theta\\in S;  A\\cdot\\theta=\\theta\\implies\\theta\\in S\nsiano a,b\\in K; siano a,b\\in K, siano X,Y\\in S\\implies AX=\\theta, \\ AY=\\theta\na\\cdot X + b\\cdot Y\\in S?\n$$$A(aX+bY)=a(AX)+b(AY)=a\\cdot\\theta+b\\cdot\\theta=\\theta\\implies a\\cdot X+b\\cdot Y\\in S$$\n\nESEMPIO\ni sottospazi di R^2 sono: \\{\\theta=(0,0)\\},\\ R^2\ngli altri sottospazi di R^2 sono tutte le rette passanti per \\theta\nPROPOSIZIONE\nsia V un K-spazio vettoriale.\nsiano U\\subseteq V e W\\subseteq V sottospazi \\implies U\\cap W è un sottospazio di V\nDIMOSTRAZIONE\n\n\\theta\\in U ;  \\theta\\in W \\implies\\theta\\in U\\cap W\nsiano a,b\\in K ; siano v,z\\in U\\cap W\\implies v,z\\in U e v,z\\in W ;\na\\cdot v+b\\cdot z\\in U;  a\\cdot v+b\\cdot z\\in W \\implies a\\cdot v+b\\cdot z\\in U\\cap W ;\n\nDEFINIZIONE (SOMMA DI SOTTOSPAZI)\nsia V un K-spazio vettoriale.\nsiano U\\subseteq V e W\\subseteq V sottospazi.\nponiamo U+W=\\{u+w \\ |\\ u\\in U, w\\in W\\}\n\nU+W si chiama somma dei sottospazi U e W\nPROPOSIZIONE\nla somma di sottospazi è un sottospazio.\nDIMOSTRAZIONE\n\n\\theta \\in U, \\theta\\in W \\implies \\theta=\\theta+\\theta\\in U+W\nsiano a,b\\in K ; siano v_{1}\\in U+W e v_{2}\\in U+W ;\nv_{1}\\in U+W \\implies v_{1}=u_{1}+w_{1} con u_{1}\\in U e w_{1}\\in U+W\nv_{2}\\in U+W\\implies v_{2}=u_{2}+w_{2} con u_{2}\\in U e w_{2}\nav_{1}+bv_{2}=a(u_{1}+w_{1})+b(u_{2}+w_{2})=au_{1}+aw_{1}+bu_{2}+bw_{2}=(au_{1}+bu_{2})+(aw_{1}+bw_{2})\n\\implies av_{1}+bv_{2}\\in U+W\n\nDEFINIZIONE (Combinazione Lineare Vettoriale)\nsia V un K-spazio vettoriale.\nsiano a_{1},\\dots,a_{n}\\in K, siano v_{1},\\dots,v_{n}\\in V.\nil vettore a_{1}v_{1}+\\dots,a_{n}v_{n} si chiama combinazione lineare dei vettori  v_{1},\\dots,v_{n}\n\\sum_{i=1}^{n}a_{i}v_{i}\nNOTAZIONE\nsia V un K-spazio vettoriale.\nsia S\\subseteq V.\nponiamo \\mathcal{L}(S)=\\{\\text{tutte le possibile combinazioni lineari di vettori in S}\\}\nPROPOSIZIONE\nsia V un K-spazio vettoriale. sia S\\subseteq V, S\\neq \\theta\\implies\\mathcal L(S) è un sottospazio di V\nDEFINIZIONE (INSIEME DI GENERATORI)\nS si chiama insieme di generatori di \\mathcal L(S) ed \\mathcal L(S) si chiama sottospazio generato da S.\nESEMPIO\nsia V un K-spazio vettoriale.\nsia S=\\{v\\}\\implies\\mathcal L(S)=\\{av\\ | \\ a\\in K\\}\nsia S=\\{u,v\\}\\implies\\mathcal L(S)=\\{au+bv \\ |\\ a,b\\in K\\}\nESEMPIO\nsia V=R^3. siano u=(1,1,0) v=(0,1,1)\nS=\\{u,v\\}; \\mathcal L(S)=\\{au+bv | a,b\\in R\\}=\\{a(1,1,0)+b(0,1,1)\\ |a,b\\in R\\}=\\{(a,a+b,b)\\ | a,b\\in R\\}\n\\begin{cases}x=a \\\\ y=a+b \\\\ z=b\\end{cases}\\implies y=x+z\\implies x-y+z=0\\implies\\mathcal L(S)=\\{(x,y,z)\\in R^{3}|x-y+z=0 \\}\nESEMPIO\nsia A=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\in R^{2,3}\nsia W=\\{(x,y,z)\\in R^{3}|\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\begin{pmatrix}x \\\\ y \\\\ z\\end{pmatrix}=\\theta\\} è un sottospazio di R^3\n\nLEZIONE 8\nOSSERVAZIONE\nsia V un K-spazio vettoriale, ovviamente V=\\mathcal L(V).\nquindi ogni spazio vettoriale ha insieme di generatori.\nDEFINIZIONE\nun K-spazio vettoriale V si dice finitamente generato se \\exists S\\subseteq V, S finito, tale che V=\\mathcal L(S).\nESEMPIO\nV=K^3 è finitamente generato.\ninfatti sia S=\\{(1,0,0),(0,1,0),(0,0,1)\\}\\implies K^{3} =\\mathcal L(S)\ninfatti sia (x,y,z)\\in K^{3}\\implies(x,y,z)=xl_{1}+yl_{2}+zl_{3}, cioè (x,y,z) è combinazione lineare di l_{1},l_{2},l_{3}\nESEMPIO\npiù in generale, sia V=K^{m,n}.\nsia E_{i,j}\\in K^{m,n} la matrice che ha 1 nel posto (i,j) e 0 altrove.\nsia \\mathcal L=\\{E_{i,j}|1\\leq j \\leq n\\}\n\\mathcal E genera K^{m,n}\\implies K^{m,n} è finitamente generato\nDEFINIZIONE\nsia V un K-spazio vettoriale.\nsiano v_{1},\\dots,v_{n}\\in V.\ndiremo che v_{1},\\dots,v_{n} sono linearmente dipendenti se esistono n scalari a_{1},\\dots,a_{n}\\in K, non tutti nulli, tali che a_{1}v_{1}+\\dots+a_{n}v_{n}=\\theta\naltrimenti si diranno linearmente indipendenti.\nESEMPIO\nsiano v_{1}=(1,2,3),v_{2}=(3,2,1),v_{3}=(1,1,1);\nv_{1}+v_{2}=(4,4,4)=4v_{3}\\implies 1v_{1}+1v_{2}-4v_{3}=\\theta \\implies v_{1},v_{2},v_{3} sono linearmente dipendenti\nCRITERIO DI INDIPENDENZA LINEARE\nsia V un K-spazio vettoriale.\nv_{1},\\dots,v_{n}\\in V sono linearmente indipendenti se e solo se:\n\nv_{1}\\neq \\theta\nv_{i}\\in\\mathcal L(v_{1},\\dots,v_{i-1}), per 2\\leq i \\leq n\n\nESERCIZIO\ndire se i vettori v_{1}=(1,1,0), v_{2}=(0,1,1),v_{3}=(1,2,3) sono linearmente indipendenti.\nDEFINIZIONE\nsia V un K-spazio vettoriale.  sia \\mathcal L\\subseteq V, \\mathcal L finito.\n\\mathcal L si dice insieme libero se i vettori in \\mathcal L sono linearmente indipendenti.\nsia \\mathcal B\\subseteq V.\ndiremo che \\mathcal B è una base di V se \\mathcal B è un insieme libero di generatori di V.\nMETODO DEGLI SCARTI SUCCESSIVI\nsia V un K-sp. vett. finitamente generato.\nsia \\mathcal G\\subseteq V un insieme finito di generatori di V (quindi V=\\mathcal L(G))\n\\implies esiste una base \\mathcal B di V, \\mathcal B\\subseteq G.\nDIMOSTRAZIONE\nsia \\mathcal G=\\{v_{1},\\dots,v_{s}\\}\nv_{1}=\\theta lo scartiamo\nv_{1}\\neq \\theta lo lasciamo\nse abbiamo scartato v_{1}, ripetiamo su v_{2} la procedura.\nse v_{1}\\neq \\theta\nv_{2}=v_{2}\\in \\mathcal L(v_{1}) lo scartiamo\nv_{2}=v_{2}\\not\\in \\mathcal L(v_{1}) lo lasciamo\ncontinuando con questa procedura alla fine otteniamo un insieme di vettori\n\\mathcal B=\\{u_{1},\\dots,u_{r}\\}\\subseteq\\mathcal G\n\\mathcal B è una base di V.\nESTENSIONE DI UN INSIEME LIBERO AD UNA BASE\nsia V un K-sp. vett., finitamente generato.\nsia L\\subseteq V un insieme libero (finito) \\implies \\exists una base \\mathcal B di V tale che \\mathcal L\\subseteq B.\nDIMOSTRAZIONE\nsia L=\\{u_{1},\\dots,u_{r}\\} un insieme libero.\nsia G=\\{v_{1},\\dots,v_{s}\\} un insieme finito di generatori di V.\nconsideriamo la “lista” di vettori: u_{1},\\dots,u_{r},v_{1},\\dots,v_{s}\na questa lista applichiamo il metodo degli scarti successivi.\nnessuno degli u_{i} si potrà scartare \\implies alla fine otterremo una base \\mathcal B tale che \\mathcal L \\subseteq \\mathcal B.\nESERCIZIO\nsiano v_{1}=(2,1,0), v_{2}=(1,1,0) in \\mathbb R^3\n\nprovare che L=\\{v_{1},v_{2}\\} è un insieme libero.\nestendere L ad una base di \\mathbb R^3\n\nLEMMA DI STEINITZ\nsia V un K-sp. vett., finitamente generato.\nsia \\mathcal G\\subseteq V un insieme finito di generatori;\nsia L\\subseteq V un insieme libero \\implies\n|L|\\leq|G|\n(|S| numero di elementi in S)\nCOROLLARIO\nsia V un K-sp. vett. fin. gen.\nsiano \\mathcal B_{1} e \\mathcal B_{2} due basi di V \\implies |\\mathcal B_{1}|=|\\mathcal B_{2}|\nDIMOSTRAZIONE\n\\mathcal B_{1} è libero\n\\mathcal B_{2} è insieme di generatori\n\\downarrow\n|\\mathcal B_{1}|\\leq|\\mathcal B_{2}|\n\\implies |\\mathcal B_{1}|=|\\mathcal B_{2}|\n|\\mathcal B_{2}|\\leq|\\mathcal B_{1}|\n\\uparrow\n\\mathcal B_{2} è libero\n\\mathcal B_{1} è insieme di generatori\nDEFINIZIONE\nsia V un K-sp. vett. fin. gen.\nsi chiama dimensione di V il numero di elementi di una base di V.\nla indichiamo con dim_{k}V\nESEMPIO\nsia V=K^{m,n}.    \\mathcal L=\\{E_{i,j}|\\ 1\\leq i\\leq m, \\ 1 \\leq j\\leq n\\} è una base di K^{m,n}, detta base canonica\n\\implies dim_{k}K^{m,n}=\\mathcal L=m\\cdot n.\nFORMULA DI GRASSMANN\nsia V un K-sp. vett. fin. gen. e siano U,W\\subseteq sottospazio\n\\implies dim_{k}(U+W)=dim_{k}U+dim_{k}W-dim_{k}(U\\cap W)\nPROPOSIZIONE\nsia V un K-sp. vett. fin. gen.\nsia W\\subseteq V un sottosp.\n\ndim_{k}W\\leq dim_{k}V\ndim_{k}W = dim_{k}V \\iff W=V\n\nDIMOSTRAZIONE\n\nsia \\mathcal B_{W} una base di W \\implies \\mathcal B_{W} è un insieme libero in V\n\\implies   \\mathcal B_{W} si può estendere ad un base \\mathcal B_{V} di V\n\\implies \\mathcal B_{W}\\subseteq\\mathcal B_{V} \\implies |\\mathcal B_{W}|\\leq|\\mathcal B_{V}|\\implies dim_{k}W\\leq dim_{k}V\nsupponiamo che dim_{k}W=dim_{k}V.  siano \\mathcal B_{W}\\subseteq \\mathcal B_{V}, \\mathcal B_{W} base di W e \\mathcal B_{V} base di V.\n|\\mathcal B_{W}|=|\\mathcal B_{V}|\\implies \\mathcal B_{W}=\\mathcal B_{V}\\implies\\mathcal L(\\mathcal B_{W})=\\mathcal L(\\mathcal B_{V})\\implies W=V\n\nviceversa, se W=V \\implies dim_{k}W=dim_{k}V\nDEFINIZIONE\nsia A\\in K^{m,n}. \\implies A=\\begin{pmatrix}R_{1} \\\\ \\dots \\\\R_{m}\\end{pmatrix}=(C_{1},\\dots,C_{n})\nR_{i}\\in K^n riga di A, C_{j}\\in K^m colonna di A.\nsi chiama spazio delle righe di A, il sottospazio \\mathcal L_{R}(A)=\\mathcal L(R_{1},\\dots,R_{m})\nsi chiama spazio delle colonne di A, il sottospazio \\mathcal L_{C}(A)=\\mathcal L(C_{1},\\dots,C_{n})\nESEMPIO\nsia A=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\in R^{2,3}\n\\mathcal L_{c}(A)=\\mathcal L((1,4),(2,5),(3,6))\\subseteq R^2;  \\mathcal L_{r}(A)=\\mathcal L((1,2,3),(4,5,6))\\subseteq R^3\nTEOREMA DI KRONECKER\nsia A\\in K^{m,n}\\implies dim_{k}\\mathcal L_{r}(A)=dim_{k}\\mathcal L_{c}(A)=rk A\n\nTEOREMA DI ROUCHÉ-CAPELLI\nsia AX=B un sistema lineare. esso è risolubile \\iff rkA=rk(A|B)\nDIMOSTRAZIONE\n\nPROPOSIZIONE\nsia A\\in K^{m,n}.\nsia W=\\{X\\in K^{n}|AX=0\\}\\subseteq K^n.\nsappiamo che W è un sottospazio di K^n.\ndim_{k}W=n-rkA\nESEMPIO\n\nPROPOSIZIONE\nsia A\\in K^{n-1,n}, con rkA=n-1\nsia W=\\{X\\in K^{n}|AX=0\\}\n\ndim_{k}W=n-(n-1)=1\nuna base di W è il vettore le cui componenti sono i minori a segno alterno di A.\n\nLEZIONE 9\nDEFINIZIONE\nsia A\\in K^{m,n}, sia M un minore di A di ordine r.\nsi chiama orlato di M un minore di ordine r+1, contenente M.\nTEOREMA\nsia A\\in K^{m,n}, rkA=r se e solo se:\n\nA contiene un minore M, di ordine r, non nullo\ntutti gli orlati di M sono nulli.\n\nApplicazioni Lineari\nLEZIONE 10\nDEFINIZIONE\nsiano U e V K-spazi vettoriali.\nsia U\\to V una funzione.\ndiremo che \\mathcal f è un’applicazione lineare se:f(a_{1}u_{1}+a_{2}u_{2})=a_{1}f(u_{1})+a_{2}f(u_{2}), \\ \\forall a_{1},a_{2}\\in K; \\ \\forall u_{1},u_{2}\\in U\nESEMPIO\nsia A\\in K^{m,n}.\nallora A definisce una funzione K^n\\to K^m, f_{A}(X)=AX\\in K^m\nF_{A} è un applicazione lineare, infatti:\nsiano b_{1},b_{2}\\in K,\\ x_{1},x_{2}\\in K^n\nf_{A}(b_{1}X_{1}+b_{2}X_{2})=A(b_{1}X_{1}+b_{2}X_{2})=A(b_{1}X_{1})+A(b_{2}X_{2})=b_{1}(AX_{1})+b_{2}(AX_{2})=b_{1}f_{A}(X_{1})+b_{2}f_{A}(X_{2})\nESEMPIO\nsia A=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\in \\mathbb R^{2,3}\\implies A definisce un applicazione lineare \\mathbb R^{3}\\to\\mathbb R^2 .\nvediamo come F_{A} agisce sui vettori della base canonica di \\mathbb R^3, \\mathcal E .\n\\mathcal E=\\{l_{1},l_{2},l_{3}\\} con l_{1}=(1,0,0), \\ l_{2}=(0,1,0), \\ l_{3}=(0,0,1)\nF_{A}(l_{1})=\\begin{pmatrix}1 &amp; 2 &amp; 3  \\\\  4 &amp; 5 &amp; 6\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}=C_{1}\nF_{A}(l_{2})=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}=\\begin{pmatrix}2 \\\\ 4\\end{pmatrix}=C_{2}\nF_{A}(l_{3})=\\begin{pmatrix}1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6\\end{pmatrix}\\begin{pmatrix}0 \\\\ 0 \\\\ 1\\end{pmatrix}=\\begin{pmatrix}3 \\\\ 6\\end{pmatrix}=C_{3}\nPROPOSIZIONE\nsia U\\to V un applicazione lineare.\n\nDIMOSTRAZIONE\nf(\\theta_{U})=f(\\theta_{U}+\\theta_{U})=f(\\theta_{U})+f(\\theta_{U})\n\\implies f(\\theta_{U})=f(\\theta_{U})+f(\\theta_{U})\\implies\\theta_{V}=f(\\theta_{U})\nIMMAGINE E CONTROIMMAGINE\nsiano A e B due insiemi, e sia A\\to B una funzione.\n\nsia X\\subseteq A.\nsi chiama immagine di X, l’insieme:\nf(X)=\\{f(x)|\\ x\\in X\\}\nin particolare se X=A, poniamo \\mathcal {Im}f=f(A), detta immagine di f\nricordiamo inoltre che f si dice suriettiva se \\mathcal Imf=B.\n\nsia Y\\subseteq B.\nsi chiama controimmagine di Y, l’insieme:\nf^-1=\\{a\\in A|\\ f(a)\\in Y\\}\nESEMPIO\nsia \\mathbb R\\to \\mathbb R, \\ \\ f(x)=x^3-x\nsia Y=\\{0\\}   f^-1(\\{0\\})=\\{x\\in \\mathbb R| \\ f(x)\\in \\{0\\}\\}=\\{x\\in\\mathbb R| \\ x^3-x=0\\};\nx^{3}-x=0\\iff x(x^{2}-1)=0\\iff x=0,1,-1\n\\implies f^-1(\\{0\\})=\\{-1,0,1\\}\nsia Z=[0,+\\infty[\\subseteq \\mathbb R\nf^-1(Z)=\\{x\\in\\mathbb R |\\ f(x)\\in[0,+\\infty[\\ \\}=\\{x\\in\\mathbb R |x^3-x\\in[0,+\\infty[\\ \\}=\\{x\\in\\mathbb R|\\ x^{3}-x\\geq 0\\}=[-1,0]\\cup[1,+\\infty[\nx^{3}-x\\geq 0       x(x^{2}-1)\\geq 0\n\nPROPOSIZIONE\nsia U\\to V un applicazione lineare.\n\nsia W\\subseteq U un sottospazio \\implies f(W) è un sottospazio di V\nsia Z\\subseteq V un sottospazio \\implies f^{-1}(Z) è un sottospazio di U\n\nDIMOSTRAZIONE\n\n\n\\theta_{V}\\in f(W)? poiché W è un sottospazio di U, \\theta_{U}\\in W\\implies\\theta_{V}=f(\\theta_{U})\\in f(W)\nresta da dimostrare che f(W) è chiuso rispetto alle combinazioni lineari\nsiano a,b\\in K, siano 1v_{1},v_{2}\\in f(W)\\implies v_{1}=f(W_{1}),v_{2}=f(W_{2})\nav_{1}+bv_{2}=af(W_{1})+{(W_{2}})=f(aw_{1}+bw_{2})\\in f(W).\n\n\n\\theta_{U}\\in f^-1(Z)?\nf(\\theta_{U})=\\theta_{V}\\in Z, perché Z è un sottospazio di V.\nsiano a,b\\in K\n\n"},"Architettura-Degli-Elaboratori":{"slug":"Architettura-Degli-Elaboratori","filePath":"Architettura Degli Elaboratori.md","title":"Architettura Degli Elaboratori","links":[],"tags":[],"content":"Indice\nINTRODUZIONE\nMEMORIA\nBUS\nI/O\nPROCESSORE\nLIVELLO SOFTWARE\nPIPELINE\nSTRUTTURA BASE CPU\nINTRODUZIONE\nun’algoritmo è una serie di istruzioni ben definite, codificate in un linguaggio x che mira al risolvere un dato problema.\nCon “Architettura del calcolatore” intendiamo l’insieme della unità principali che compongono il calcolatore e come interagiscono tra di esse.\nLe funzioni base di un calcolatore possono essere suddivise in:\n\nMemorizzare dei dati\nElaborare dei dati\nTrasferire dei dati\nControllo\n\nIL PROCESSORE\nIl processore è costituito da 4 parti fondamentali:\n\nIl Program Counter, che è una locazione di memoria contenente l’indirizzo dell’istruzione da eseguire.\nIl Registro delle Istruzioni, che è una locazione di memoria contenente l’istruzione da eseguire.\nL’unità logico-aritmetica, che fa i calcoli.\nL’unità di controllo, che fa eseguire l’istruzione tramite i cambiamenti di stato.\n\nLa Memoria Centrale\nLa memoria si suddivide in due categorie:\n\nVolatile\nNon Volatile\nLa memoria volatile è un tipo di memoria che “perde” i dati quando il calcolatore viene spento, ed anch’essa si suddivide in due componenti:\nRAM (Random access memory), che immagazzina i programmi in esecuzione.\nCACHE, che immagazzina i dati più utilizzati per tenerli pronti al momento di riutilizzarli.\nLa memoria non volatile è la ROM (Read only Memory), che immagazzina i dati a lungo termine, anche a spegnimento del calcolatore.\n\nMACCHINA DI VON NEUMANN\nLa macchina di Von Neumann è un concetto diverso rispetto alla MdT, infatti si focalizza sul concetto “pratico” della risoluzione di un algoritmo, essa infatti è composta da:\n\nCPU\nMEMORIA\nBUS\nI/O\nMEMORIA DI MASSA\n\n\nA livello teorico, è una terna di {N, IS, P}:\n\nN = \\{0,1,2,3...\\}, è l’insieme dei numeri naturali (rappresenta l’alfabeto della macchina).\nIS = \\{ZERO, INC, SOM, SOT, MOL\\}, è l’istruction set, ovvero l’insieme delle istruzioni generiche della macchina.\nP =  {I_0, I_1, I_2, I_3,...,I_p}   , è una sequenza finita e non vuota di istruzioni prese da IS, con valori specifici della variabili, andando a comporre un programma.\n\n\nLayer fondamentali\nL’architettura è un “contratto tra hardware e software” (se l’architettura è 32bit/64bit, il sistema operativo o le applicazioni devo corrispondere alla versione dell’architettura).\nl’implementazione riguarda invece come l’architettura viene realizzata fisicamente.\nl’informatica è suddivisa in 3 layer fondamentali:\n\nAPPLICAZIONI\nSISTEMA OPERATIVO\nHARDWARE\nL’architettura di von neumann è tutt’oggi il fondamento architetturale.\nil bottleneck di von neumann è un effetto causato dalla differenza di banda tra CPU e memoria, come se la latenza della memoria facesse da “tappo” alla cpu.\n\nLA PIPELINE\nla pipeline è la catena di distribuzione:\n\nFETCH (recupero dell’istruzione della memoria)\nDECODE (decodifica e interpreta l’istruzione)\nEXECUTE (escuzione dell’operazione)\nMEMORY (accesso alla memoria)\nWRITEBACK (salva nei registri)\n\nerrori con cui si ci può scontrare nella pipeline:\n\nData Hazard\n\nx = a+b\ny = x+c\ny dipende da x, quindi si può avere un contrasto di dato\n\nControl Hazard\ntramite comandi come il GOTO, alteriamo il flusso del programma, portando alla riesecuzione della pipeline, ma si può anche generare un data hazard\nStructural Hazard\ni processi si “battono” per avere le risorse della cpu\n\nHazard\nQuando più istruzioni vengono eseguite in parallelo, possono verificarsi hazard, cioè conflitti che impediscono l’esecuzione simultanea corretta.\nTipologie principali\n\nData Hazard → un’istruzione dipende dal risultato di un’altra non ancora completata.Esempio: B usa un valore che A deve ancora calcolare.\nStructural Hazard → due istruzioni vogliono usare la stessa risorsa hardware.Esempio: entrambe vogliono accedere all’unità di moltiplicazione.\nControl Hazard → causati dai salti condizionali, dove il processore non sa ancora quale direzione prendere.\n\n\nI processori moderni riducono gli hazard tramite riordino dinamico e predizione dei salti.\n\n\nEsecuzione Fuori Ordine (Out-of-Order Execution)\nCon questa tecnica, il processore riorganizza dinamicamente le istruzioni per sfruttare al massimo le proprie risorse.\nSe un’istruzione deve aspettare un dato, il processore può eseguirne un’altra indipendente nel frattempo.\nLe strutture come la Reorder Buffer assicurano che i risultati vengano comunque resi visibili nell’ordine corretto, garantendo coerenza del programma.\n\nBranch Prediction\nI salti condizionali (branch) introducono incertezza nel flusso del programma.\nLa branch prediction tenta di indovinare in anticipo quale percorso verrà preso.\nI processori moderni:\n\nusano predittori storici o reti neurali,\nraggiungono accuratezze superiori al 95%.\n\n\nSe la predizione è sbagliata, entra in gioco il rollback per ripristinare lo stato precedente.\n\n\nROLLBACK\nIl rollback permette di annullare le istruzioni speculative e ripristinare lo stato corretto del processore.\nÈ analogo ai punti di ripristino di un sistema operativo: se qualcosa va storto, si torna indietro all’ultimo stato valido.\nIL MICROPROCESSORE\nil microprocressore è stato una rivoluzione, avendo un’intera cpu su un singolo chip di silicio, portando l’informatica in tutti i contesti, tra cui il personal computer.\nulteriore agevolazione dall’integrazione in un singolo chip, si elimina il BUS e quindi riduciamo drasticamente il rallentamento provocato da esso, evitando il bottleneck.\nEVOLUZIONE ISA (ISTRUCTION SET ARCHITECTURE)\n\nCISC (complex istruction set computer)\nistruzioni complesse che eseguono operazioni elaborate in un singolo comando.\nesempi sono: x86 intel e amd\nRISC (REDUCED ISTRUCTION SET COMPUTER)\nset di istruzioni semplificate ma efficienti.\nesempi sono: ARM per i dispositivi mobile, MIPS per sistemi embedded\nle architetture moderne utilizzano un sistema ibrido tra CISC e RISC\n\nARCHITETTURE SUPERSCALARI\nLe architetture superscalari rappresentano un’importante evoluzione nel design dei processori moderni. Questi processori sono caratterizzati dalla capacità di eseguire simultaneamente molteplici istruzioni durante ogni singolo ciclo di clock, sfruttando il parallelismo a livello di istruzioni per migliorare significativamente le prestazioni complessive del sistema.\nSUPERPIPELINING\nIl superpipelining rappresenta una tecnica avanzata che consiste nell’aumentare significativamente il numero di stadi che compongono la pipeline del processore. Attraverso questa suddivisione più granulare, ogni singolo stadio della pipeline viene ridotto in termini di complessità e durata, permettendo così di raggiungere frequenze di clock molto più elevate rispetto alle architetture tradizionali.\n\nEntrambe le tecnologie, sia i processori superscalari che il superpipelining, contribuiscono in modo sostanziale all’aumento del throughput complessivo del sistema, ovvero la quantità totale di istruzioni che possono essere elaborate nell’unità di tempo.\n\nIssue Width\nL’issue width rappresenta il numero massimo di istruzioni che un processore superscalare può inviare in esecuzione in un singolo ciclo di clock.\nAd esempio, se un processore ha un’issue width di 4, significa che può potenzialmente eseguire fino a 4 istruzioni nello stesso ciclo.\n\nAumentare la issue width migliora il parallelismo, ma aumenta anche la complessità e il rischio di conflitti tra istruzioni.\n\nEra Multicore\nQuando i limiti fisici (calore, consumo) hanno bloccato l’aumento delle frequenze, si è passati dai processori single-core ai multicore.\nIn questo modo più core lavorano in parallelo, aumentando le prestazioni tramite parallelismo spaziale.\nIl problema principale è la coerenza della memoria condivisa.\nPer risolverlo, vengono usati protocolli come:\n\nMESI (Modified, Exclusive, Shared, Invalid) → garantisce che tutti i core abbiano una visione coerente della memoria.\n\n\nGPU e Parallelismo Massivo\nLe GPU (Graphics Processing Unit) portano il concetto di parallelismo all’estremo.\nA differenza delle CPU (pochi core potenti), le GPU hanno centinaia o migliaia di core leggeri, ideali per eseguire molti calcoli in parallelo.\nSono perfette per:\n\nelaborare immagini (matrici R-G-B),\nmachine learning,\nsimulazioni scientifiche.\n\n\nLe GPU privilegiano il throughput elevato (tante operazioni contemporanee) piuttosto che la bassa latenza.\n\nArchitetture ibride ed Eterogenee\nL’evoluzione verso architetture ibride rappresenta la risposta moderna e più avanzata alle crescenti esigenze di efficienza energetica e di prestazioni altamente specializzate nei sistemi di calcolo contemporanei. L’integrazione strategica di più processori diversificati e componenti di elaborazione su un singolo chip consente di ottimizzare in modo significativo il consumo energetico complessivo del sistema e di migliorare notevolmente le prestazioni complessive per tipologie di lavori specifici e carichi computazionali mirati.\n\n\nL’offload computazionale permette alla CPU di delegare compiti specifici agli acceleratori più efficienti, dato che gli acceleratori dedicati sono 10-100 volte più efficienti.\n\nMEMORIA\nLa memoria di un calcolatore può essere vista come un contenitore ordinato di celle, ognuna delle quali può contenere un dato.\nOgni cella di memoria ha una dimensione di 1 byte (8 bit) e possiede un indirizzo univoco che serve per localizzarla e accedere al valore che contiene.\nGli indirizzi partono sempre da zero, quindi l’indirizzo dell’ultima cella corrisponde al numero totale di celle meno uno.\nAd esempio, una memoria composta da 1024 celle avrà indirizzi che vanno da 0 a 1023.\n\nL’ampiezza dello spazio di indirizzamento dipende dal numero di linee del bus indirizzi del sistema.\nIn generale, se il bus indirizzi ha n linee, lo spazio di indirizzamento sarà pari a: 2^n\nQuesto valore indica quante celle di memoria è possibile indirizzare.\nAd esempio:\n\ncon 16 linee di indirizzi → (2^{16} = 65.536) locazioni (64 KB);\ncon 32 linee → (2^{32} = 4.294.967.296) locazioni (4 GB).\n\nIn un sistema operativo moderno, la memoria può essere riassegnata a più processi, grazie alle tecniche di allocazione dinamica e memoria virtuale.\nUn esempio pratico è il file system FAT32, che non può gestire file superiori a 4 GB, ma può accettare più trasferimenti da dimensioni inferiori (ad esempio, due file da 3 GB ciascuno).\n\nIl contenuto della memoria principale rimane memorizzato solo finché il computer è alimentato.\nQuando l’alimentazione viene interrotta, i dati vengono persi: per questo motivo la memoria RAM è detta volatile.\n\nAll’interno della memoria di sistema è presente una sezione fondamentale: il BIOS (Basic Input/Output System).\nIl BIOS contiene due fasi principali:\n\nBootstrap – avvia il sistema e immette sul bus le prime istruzioni necessarie per inizializzare i dispositivi di base;\nPOST (Power-On Self Test) – esegue i controlli di base e carica il sistema operativo.\n\nLe aree di memoria che contengono il BIOS e altri firmware sono realizzate in tecnologia ROM (Read Only Memory), che è non volatile: mantiene i dati anche senza alimentazione.\n\nEsistono due principali tipi di memoria RAM:\n\n\nDRAM (Dynamic RAM): più lenta ma economica, utilizzata come memoria principale del sistema.\nI moduli DRAM sono installati negli slot DIMM della scheda madre.\n\n\nSRAM (Static RAM): più veloce ma anche molto più costosa.\nPer questo motivo è usata solo per memorie temporanee ad alte prestazioni, come la memoria cache del processore.\nCache e Gerarchie di Memoria\nLa memoria principale (RAM) è molto più lenta rispetto al processore.\nPer ridurre questa latenza, si usano le cache, memorie piccole ma velocissime.\nLivelli di cache\n\nL1: la più veloce, ma piccola e dedicata a ciascun core.\nL2: intermedia per velocità e capacità.\nL3: condivisa tra i core, più grande ma più lenta.\n\nQuando un dato non è presente nella cache (cache miss), deve essere recuperato dalla RAM, rallentando l’esecuzione.\nPolitiche di sostituzione\n\nLRU (Least Recently Used): rimuove il dato usato meno recentemente.\nFIFO (First In, First Out): elimina il dato più vecchio.\nRandom: scelta casuale, utile quando si vuole semplicità di implementazione.\n\n\nMemoria Virtuale\nLa memoria virtuale consente a ogni programma di vedere uno spazio di memoria continuo e più ampio rispetto a quello fisico disponibile utilizzando il disco fisso.\nIl sistema operativo traduce gli indirizzi virtuali in indirizzi fisici reali tramite la MMU (Memory Management Unit).\n\nla memoria virtuale è più lenta rispetto alla RAM\n\nTecniche di mappatura\n\n\nPaginazione: la memoria è divisa in blocchi di dimensione fissa (pagine).\n\n\nSegmentazione: la memoria è suddivisa in segmenti logici di dimensione variabile.\n\n\nIbridi: combinano entrambi i metodi.\n\n\n\nGrazie alla memoria virtuale, parte del disco può essere usata come memoria di supporto (swap).\n\nCache e Gerarchie di Memoria\nLa memoria principale (RAM) è molto più lenta rispetto al processore.\nPer ridurre questa latenza, si usano le cache, memorie piccole ma velocissime.\nLivelli di cache\n\nL1: la più veloce, ma piccola e dedicata a ciascun core.\nL2: intermedia per velocità e capacità.\nL3: condivisa tra i core, più grande ma più lenta.\n\nQuando un dato non è presente nella cache (cache miss), deve essere recuperato dalla RAM, rallentando l’esecuzione.\nPolitiche di sostituzione\n\nLRU (Least Recently Used): rimuove il dato usato meno recentemente.\nFIFO (First In, First Out): elimina il dato più vecchio.\nRandom: scelta casuale, utile quando si vuole semplicità di implementazione.\n\n\nMemoria Virtuale\nLa memoria virtuale consente a ogni programma di vedere uno spazio di memoria continuo e più ampio rispetto a quello fisico disponibile utilizzando il disco fisso.\nIl sistema operativo traduce gli indirizzi virtuali in indirizzi fisici reali tramite la MMU (Memory Management Unit).\n\nla memoria virtuale è più lenta rispetto alla RAM\n\nTecniche di mappatura\n\n\nPaginazione: la memoria è divisa in blocchi di dimensione fissa (pagine).\n\n\nSegmentazione: la memoria è suddivisa in segmenti logici di dimensione variabile.\n\n\nIbridi: combinano entrambi i metodi.\n\n\n\nGrazie alla memoria virtuale, parte del disco può essere usata come memoria di supporto (swap).\n\n\n\nBUS\nIl bus è un insieme di fili (chiamati linee) che collegano tra loro le varie parti del computer:\n\nil processore (CPU),\nla memoria,\ne i dispositivi di input/output (I/O).\n\nIn pratica, è come una strada a più corsie su cui viaggiano i dati.\nOgni filo del bus trasporta un bit (0 o 1).\nTutti i componenti principali del computer “si affacciano” su questa strada per leggere, scrivere o scambiare informazioni.\nIl modello di von Neumann prevede che:\n\ni dati e le istruzioni risiedano in memoria,\nil processore li legga, li elabori e li riscriva,\ne che tutto questo avvenga tramite il bus.\n\nQuindi, la maggior parte delle operazioni di un computer sono trasferimenti di bit tra CPU, memoria e dispositivi di I/O.\nil processore è il Master (cioè quello che comanda),\nmentre memoria e I/O sono gli Slave (cioè quelli che eseguono le richieste).\nIl bus è il “mezzo di trasporto” che collega tutto.\nCi sono due tipi principali di trasferimenti:\n\nScrittura (Write): il processore invia dati alla memoria o ai dispositivi di I/O.\nLettura (Read): il processore riceve dati dalla memoria o dagli I/O.\n\nPer gestire queste operazioni, il bus è diviso in tre sottoblocchi principali:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipo di BusNomeFunzioneBus degli indirizziAddress Bus (ABus)Specifica dove leggere o scrivere in memoria.Bus dei datiData Bus (DBus)Trasporta i dati veri e propri.Bus di controlloControl Bus (CBus)Indica cosa fare e quando (es. leggere, scrivere, fine operazione).\nIl Control Bus contiene linee che dicono:\n\nse l’operazione è tra memoria e CPU oppure tra I/O e CPU;\nse è una lettura (R/W = 1) o scrittura (R/W = 0);\nse il trasferimento è finito (Wait = 1) o in corso (Wait = 0).\n\nIl bus ha anche un clock (una specie di metronomo) che regola il tempo dei trasferimenti.\nTutto avviene in modo sincronizzato: prima si indica l’indirizzo, poi il tipo di operazione, poi si inviano o ricevono i dati.\nIl segnale Wait serve perché la memoria è più lenta della CPU → questo crea il famoso collo di bottiglia di von Neumann: la CPU deve aspettare la memoria.\nOgni bus ha un certo numero di linee (fili).\n\n\nL’Address Bus determina quanta memoria può essere indirizzata.\nEsempio: con 16 linee si possono indirizzare (2^{16} = 65.536) locazioni (cioè 64 KB).\n\n\nIl Data Bus indica quanti bit si possono trasferire per volta (cioè il “grado di parallelismo”).\nEsempio: un DBus a 32 linee trasferisce 32 bit (4 byte) alla volta.\n\n\nSulla scheda madre del computer il bus di sistema è realizzato dal Chipset, cioè un insieme di circuiti che collegano tutto.\nSi divide in due parti:\n\nNorthBridge → collega la CPU con la memoria e la scheda video.\nSouthBridge → collega la CPU con le periferiche (tastiera, USB, disco, ecc.).\n\nNel tempo i bus sono diventati sempre più veloci:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStandardLarghezzaFrequenzaNoteISA8 bit8,33 MHzBus storico, lentoEISA16 bit8,33 MHzMigliorato ma ancora lentoPCI32 bit33 MHzBus standard per anniPCI-X32–64 bit66 MHzFino a 1 GB/sAGPdedicato alla scheda videoPCI Express (PCIe)bus moderno e velocesostituisce AGP e PCI\nIl numero di linee del bus non sempre coincide con la dimensione dell’architettura (32 o 64 bit), ma spesso è simile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArchitetturaAddress BusMemoria indirizzabileData BusDati per trasferimento32 bit32 linee2^{32} = 4 GB32 linee4 byte64 bit64 linee2^{64} \\approx 16 \\ exabyte64 linee8 byte\nNon sempre serve avere tutte le linee teoriche.\nPer esempio, un processore a 64 bit può avere solo 48 linee di indirizzi, sufficienti per gestire 256 TB di memoria fisica.\nLe scelte su quanti fili usare dipendono da:\n\nefficienza,\ncosti,\ne requisiti del sistema.\n\nI/O\nLa sezione Input/Output (I/O) di un computer serve per:\n\nacquisire dati e programmi dall’esterno (input);\nrappresentare i risultati delle elaborazioni (output).\n\nLe forme di rappresentazione sono molteplici: sullo schermo, tramite la stampante, o come dati memorizzati su dispositivi di massa (hard disk, DVD, pendrive, ecc.).\n\nDal punto di vista concettuale, la sezione di I/O si può immaginare come una memoria con celle, ma con uno spazio di indirizzamento separato e più piccolo rispetto alla memoria principale.\nOgni dispositivo di I/O (come tastiera, monitor o stampante) possiede:\n\nun proprio intervallo di indirizzi riservato, chiamato registri di I/O o porte di I/O;\nquesti indirizzi servono al processore per comunicare con quella specifica periferica.\n\n\nAlcuni dispositivi richiedono un grande spazio di I/O e utilizzano indirizzi di memoria normali invece degli indirizzi di I/O.\nIn questo caso si parla di dispositivi mappati in memoria (Memory-Mapped I/O).\nQuesta tecnica permette di trattare la periferica come se fosse una parte della memoria principale, semplificando le operazioni di lettura e scrittura.\n\nPer gestire in modo efficiente la comunicazione tra CPU e periferiche, esistono linee di sincronizzazione dedicate, chiamate linee di interruzione o interrupt.\nSul Control Bus (CBus) sono presenti due segnali principali:\n\nINTR: richiesta di interruzione da parte di una periferica;\nINTA: risposta del processore che accetta e gestisce l’interruzione.\n\nQuando arriva un interrupt, il processore sospende temporaneamente ciò che sta facendo per eseguire una procedura specifica chiamata ISR (Interrupt Service Routine).\nQuesto meccanismo è essenziale per i dispositivi che funzionano in modo asincrono, cioè che possono richiedere attenzione in qualsiasi momento (come il mouse o la tastiera).\n\nNon sempre conviene che il processore gestisca direttamente ogni trasferimento di I/O, perché ciò rallenterebbe l’esecuzione.\nPer questo motivo esistono modalità speciali:\n\nDMA (Direct Memory Access): il trasferimento dei dati avviene direttamente tra la memoria e la periferica, senza coinvolgere la CPU.\nBus Mastering: una variante del DMA in cui la periferica può diventare “padrone del bus” per un certo tempo e gestire i trasferimenti autonomamente.\n\nQueste tecniche permettono di spostare grandi quantità di dati senza occupare la CPU.\n\nOgni periferica dispone di una circuiteria chiamata scheda controller, che ha il compito di:\n\ncollegarsi al bus di sistema;\ngestire i propri indirizzi di I/O;\nsincronizzare i trasferimenti tramite il Control Bus;\ngestire eventuali linee di interrupt o DMA.\n\nLe periferiche interne (come scheda video, rete, tastiera) hanno la scheda controller integrata nel Chipset della scheda madre.\nEsistono anche controller dedicati per gestire in modo centralizzato le interruzioni e il DMA: l’Interrupt Controller e il DMA Controller.\n\nCon l’introduzione del bus PCI, che collega CPU, memoria e I/O attraverso il SouthBridge, si è semplificata la gestione degli indirizzi e dei canali DMA.\nPrima del PCI, le schede controller dovevano essere configurate manualmente (tramite ponticelli o interruttori) per assegnare indirizzi, linee di interrupt e canali DMA, evitando conflitti con altre schede.\nIl PCI ha introdotto la tecnologia Plug &amp; Play, grazie alla quale:\n\nil BIOS, il sistema operativo e il firmware della scheda controller stabiliscono automaticamente gli indirizzi e le linee di comunicazione;\nl’utente non deve più configurare manualmente i dispositivi;\nè possibile collegare dispositivi “a caldo” (senza spegnere il computer).\n\n\nLe periferiche esterne si collegano al bus del sistema tramite connettori standard.\nEsempi:\n\nSlot PCI o PCIe: per collegare schede aggiuntive (come una seconda scheda di rete o una scheda audio dedicata);\nConnessioni IDE/EIDE (ATA-ATA2): usate in passato per hard disk e DVD, oggi quasi del tutto sostituite da interfacce più moderne.\n\n\nGli standard moderni per il collegamento dei dispositivi di I/O sono:\n\nUSB (Universal Serial Bus): consente il collegamento e l’installazione “a caldo” (Hot Swap), supporta alte velocità (480 Mbit/s per USB 2.0) e fornisce alimentazione ai dispositivi;\nFireWire (IEEE 1394): simile all’USB, con velocità di 400 Mbit/s e possibilità di connessioni multiple in cascata;\nEthernet (802.x): standard per la rete locale.\n\nGli standard più vecchi come RS232, porte parallele Centronics e PS/2 sono stati progressivamente abbandonati o convertiti in connessioni USB.\nPROCESSORE\nUn processore (CPU – Central Processing Unit) è un circuito integrato in grado di eseguire operazioni logiche, aritmetiche e di controllo sui dati.\nÈ composto da varie unità funzionali, ciascuna con un ruolo specifico.\n\nCOMPONENTI PRINCIPALI\n\n\nRegistri: piccole memorie interne alla CPU, molto veloci, che contengono temporaneamente dati e indirizzi.\nServono per trasferire i dati all’interno del processore, in particolare verso l’ALU (Arithmetic Logic Unit) per l’elaborazione.\n\n\nALU (Arithmetic Logic Unit): esegue operazioni aritmetiche (addizione, sottrazione, moltiplicazione, divisione) e logiche (AND, OR, NOT, XOR).\nÈ l’unità di esecuzione effettiva del processore.\n\n\nUnità di controllo (Control Unit): gestisce il flusso delle istruzioni, decodifica gli opcode, coordina le operazioni tra registri, ALU e memoria.\n\n\nProgram Counter (PC): registro speciale che contiene l’indirizzo dell’istruzione successiva da eseguire. Dopo ogni istruzione, viene incrementato automaticamente della lunghezza dell’istruzione appena completata.\n\n\n\nDATA PATH\nIl data path rappresenta il percorso che i dati compiono all’interno del processore:\n\\text{Registri} \\rightarrow \\text{Registri di input} \\rightarrow \\text{ALU} \\rightarrow \\text{Registro di output}\\rightarrow registri\nQuesto ciclo consente il trasferimento e l’elaborazione continua delle informazioni.\n\nCICLO DI ISTRUZIONE DEL PROCESSORE\nIl processore opera secondo una sequenza ciclica di fasi, nota come FETCH – DECODE – EXECUTE – STORE:\n\n\nFETCH (Prelievo):\nL’unità di controllo pone sul bus degli indirizzi (Address Bus) il valore contenuto nel Program Counter, per leggere dalla memoria l’istruzione da eseguire.\nL’opcode (Operation Code) dell’istruzione viene caricato nel registro istruzioni (Instruction Register).\n\n\nDECODE (Decodifica):\nL’unità di controllo interpreta l’Op.code, determinando quali operandi servono e quale operazione eseguire.\nIn questa fase vengono caricati gli operandi dai registri o dalla memoria (Operand Fetch).\n\n\nEXECUTE (Esecuzione):\nL’ALU o un’unità funzionale specializzata esegue l’operazione richiesta sull’input.\nLa velocità di questa fase dipende dal clock della CPU, che regola la frequenza di esecuzione dei microprogrammi.\n\n\nSTORE (Memorizzazione):\nI risultati dell’operazione vengono memorizzati nei registri o scritti in memoria (tramite il bus dei dati) oppure inviati ai dispositivi di I/O.\n\n\n\nTIPOLOGIE DI ARCHITETTURA\nCISC (Complex Instruction Set Computer)\n\nOgni istruzione può eseguire operazioni complesse (ad esempio accesso in memoria + calcolo + memorizzazione).\nLe istruzioni sono numerose e variabili in lunghezza, quindi la fase di decodifica è complessa.\nRichiede un data path a più cicli per completare l’esecuzione di un’istruzione.\nÈ più compatibile e portabile a livello software: lo stesso codice Assembly può essere usato su CPU successive.\nEsempi: Intel x86, AMD64.\n\nRISC (Reduced Instruction Set Computer)\n\nUtilizza un insieme ridotto e uniforme di istruzioni, generalmente di lunghezza fissa (es. 32 bit).\nOgni istruzione viene eseguita in un solo ciclo di clock, con un data path a singolo passo.\nL’esecuzione è più rapida e prevedibile.\nPer eseguire un’operazione complessa serve una sequenza di più istruzioni semplici.\nEsempi: ARM, MIPS, RISC-V, SPARC.\n\nDifferenze principali:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaratteristicaCISCRISCNumero di istruzioniAltoBassoLunghezza istruzioneVariabileFissaFasi per istruzionePiù cicliUn solo cicloDecodificaComplessaSempliceVelocità mediaInferioreMaggioreEsempix86, 68000ARM, RISC-V\nSistema di Cache\nPer evitare il bottleneck dovuto alla lentezza della memoria principale rispetto alla CPU, si utilizza un sistema di cache.\nQuando la CPU legge un indirizzo dalla memoria, gli indirizzi successivi vengono memorizzati nella cache.\nQuesto avviene tramite un bus dedicato, molto più veloce rispetto al bus della memoria principale (più lunga è la linea del bus, minore è la velocità di trasmissione).\nIl funzionamento può essere riassunto così:\n\nLa CPU cerca il dato prima nella cache.\nSe il dato non è presente (cache miss), viene letto dalla memoria principale.\nIl dato viene quindi copiato nella cache, eventualmente rimpiazzando la linea meno utilizzata (Least Recently Used – LRU).\n\nLivelli di cache:\n\nCache L1 → all’interno della CPU (la più veloce, ma più piccola)\nCache L2 → collegata alla CPU (più grande, ma leggermente più lenta)\nCache L3 → condivisa, collegata alla scheda madre (ancora più grande, ma meno veloce)\n\n\nPrefetch\nFin dalle prime CPU, si è introdotto il prefetch:\nla CPU preleva e memorizza in anticipo alcuni byte (ad esempio 6 o 8) successivi a quelli richiesti, in modo da anticipare l’uso futuro dei dati e ridurre l’accesso al bus.\nQuesto meccanismo aumenta la probabilità che le istruzioni necessarie siano già disponibili nella cache.\n\nSuperscalar Architecture\nUn processore con più ALU (Arithmetic Logic Unit) può gestire più pipeline contemporaneamente:\n→ questo tipo di architettura si chiama superscalare.\n\nArchitetture CISC e ibridi moderni\nI processori Intel i7 e i9 appartengono alla famiglia CISC (Complex Instruction Set Computer),\nma internamente sono ibridi, poiché utilizzano una microarchitettura RISC-like per ottimizzare l’esecuzione delle istruzioni complesse.\nLIVELLO SOFTWARE\nIl Processo di Programmazione e Traduzione\nLivelli di Linguaggio\n\nI programmi sono scritti in Linguaggio ad Alto Livello (es. C, C++, ecc.), che è il più espressivo.\nQuesto codice viene tradotto in Linguaggio macchina assemblativo (Assembly).\nIl linguaggio assemblativo viene infine tradotto in Linguaggio macchina binario.\n\nStrumenti di Traduzione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrumentoTrasformazioneNoteCompilatoreCodice ad Alto Livello → Codice AssemblativoAutomatizza molti compiti del programmatore assembly. Un compilatore che riorganizza le istruzioni è detto Ottimizzante.AssemblatoreCodice Assemblativo → Codice Macchina Binario (File Oggetto)Esegue passi come la generazione della codifica binaria e la gestione delle direttive di allocazione di memoria.\nGenerazione del Programma Oggetto\nLa generazione di un file eseguibile completo (Programma Oggetto) richiede la collaborazione di più strumenti.\nFlusso di Lavoro\nCompilatore: Trasforma i file sorgenti ad alto livello in file sorgenti assembly.\nAssemblatore: Trasforma i file sorgenti assembly in File Oggetto.\nLinker (Collegatore): Collega assieme vari file oggetto e file di libreria in un unico Programma Oggetto.\n\nIl Ruolo del Linker e delle Librerie\n\nQuando un file sorgente chiama sottoprogrammi dichiarati altrove, l’assemblatore genera un file oggetto incompleto (con riferimenti a nomi esterni).\nIl Linker risolve questi riferimenti combinando più file oggetto separati.\nLe Librerie sono file che raggruppano file oggetto riutilizzabili e contengono le informazioni necessarie al Linker per risolvere i riferimenti a nomi esterni. Sono create dall’utility Archiver.\n\n\nAssemblatore a Due Passate\nSi usa un metodo a 2 passate per gestire i simboli (es. etichette dei salti in avanti) che vengono usati prima di essere dichiarati.\nPasso 1: Scorre il codice sorgente e raccoglie tutti i simboli e i rispettivi valori numerici nella tabella dei simboli.\nPasso 2: Scorre nuovamente il codice sorgente, sostituisce i simboli con i valori in tabella e genera il codice oggetto finale.\nEsecuzione e Debug del Programma\nLoader (Caricatore)\nIl Loader è il programma che carica il programma oggetto dalla memoria secondaria (disco) alla memoria centrale per l’esecuzione.\n\nLegge informazioni (lunghezza e locazione di caricamento) dall’header del file oggetto.\nCarica il programma in memoria.\nSalta alla prima istruzione del programma da eseguire.\n\nDebugger\nIl Debugger è un programma che consente di eseguire e interrompere il programma oggetto per controllare lo stato della memoria e dei registri e individuare errori di programmazione (bug).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModalitàDescrizioneMeccanismoTrace ModeIl programma viene eseguito passo-passo, interrompendosi dopo ogni istruzione.Si genera un’eccezione al termine di ogni istruzione; il Debugger è la routine di servizio dell’interruzione.BreakpointL’esecuzione si interrompe in punti di osservazione specifici definiti dal programmatore.Il Debugger sostituisce temporaneamente le istruzioni nei punti di osservazione con speciali interruzioni software (Trap).\nIl Sistema Operativo (SO)\nIl Sistema Operativo gestisce il coordinamento generale di tutte le attività del calcolatore.\nFunzioni: Esecuzione concorrente di programmi, gestione degli I/O, gestione dell’accesso alla memoria, gestione dell’interfaccia utente, etc.\nStruttura: Formato da un insieme di routine essenziali che risiedono nella memoria centrale e programmi di utilità che risiedono su disco.\nSistemi Multitasking: Sistemi operativi capaci di eseguire più programmi contemporaneamente (virtualmente).\n\nGestione Concorrente (Time Slicing): Il SO divide l’esecuzione dei programmi in quanti di tempo (\\tau). Un contatore lancia un’interruzione a intervalli regolari, attivando la routine SCHEDULER per scegliere il prossimo programma da eseguire.\n\nStati dei Programmi: RUNNING, RUNNABLE e BLOCKED.\nPIPELINE\nla pipeline serve a parallelizzare i processi della CPU durante l’esecuzione di un programma.\nil pipelining è una tecnica avanzata nei processori:\n\nil principio fondamentale è la parallelizzazione dei processi per ottimizzare tutti i processi in meno cicli di clock.\ngli stadi della pipeline sono:\nFETCH → DECODE →EXECUTE →MEMORY → WRITE\nnel caso migliore (senza cache miss) si hanno 5 istruzioni eseguite in parallelo\nper gestire l’esecuzione in pipeline di più istruzioni, è necessario mantenere informazioni tra uno stadio e l’altro.\nqueste informazioni vengono mantenute nei buffer interstadi che contengono il dato tra uno stadio e l’altro della pipeline e contengono:\n\ni registri interstadio (RA,RB,PC_TEMP, etc..)\nil registro IR (per mantenere gli identificatori dei registri sorgente e destinazione)\nsegnali di controllo per i vari stadi\n\nI PROBLEMI DEL PIPELINING:\n\nDATA HAZARD\nRITARDO ACCESSO MEMORIA\nRITARDO NEI SALTI\nLIMITE DI RISORSE\n\nDATA HAZARD\nun data hazard accade quando un istruzione contiene un registro sorgente che ancora non è stato aggiornato dalle istruzioni precedenti:\n\n\nin questo caso l’istruzione ADD aggiornerà il contenuto di R2 alla fine della fase WRITE\nl’istruzione subtract legge il contenuto di R2 mentre si trova in fase di EXECUTE\nl’istruzione subtract resterà in stallo finché R2 non sarà aggiornato (3 cicli di clock)\n\ni vari tipi di data hazard sono:\n\n\nRAW\nR2 legge il contenuto di R1 prima che l’istruzione su R1 abbia finito la fase MEMORY, andando a leggere un valore errato\n\n\nWAW\nistruzione i salva un operando in R3 prima che lo salvi istruzione j, lasciando così solo l’istruzione j\n\n\nWAR\nR1 viene cambiato nell’istruzione i e nell’istruzione j viene letto\n\n\nISTRUZIONI NOP\nper mettere un istruzione in stallo si utilizzano le istruzioni nop tra le due istruzioni.\nciascuna NOP crea un ciclo di inattività chiamato bolla.\nle istruzioni nulle possono essere generate via compilatore o via hardware tramite circuiti complessi.\nè una tecnica usata per evitare i ritardi causati dai data hazard, con questa tecnica indirizziamo direttamente gli operandi dell’istruzione precedente senza doverli pprima salvare in memoria\ni compilatori ottimizzanti possono spostare istruzioni utili nelle posizioni delle NOP\nINOLTRO DEGLI OPERATORI (FORWARDING)\npper risolvere il problema si ricorre all’operand forwarding dove alla fine di istruzioni 1 il dato viene rimandato direttamente alla ALU della prossima istruzione, saltando quindi il WRITEBACK e dei possibili stalli.\n\n\nRitardi della memoria\nGli accessi alla memoria principale possono richiedere un numero di cicli di clock superiore a quello necessario per gli accessi alla cache. Nei casi di cache miss, il dato richiesto non risiede nella cache e deve essere recuperato dalla memoria centrale, introducendo ritardi dell’ordine di dieci o più cicli di clock.\nPoiché la pipeline è organizzata in stadi sequenziali, un’istruzione che permane nello stadio di accesso alla memoria impedisce l’avanzamento delle istruzioni successive, causando un blocco globale della pipeline per tutta la durata del ritardo.\n\nRitardi nei salti\nLe istruzioni di salto condizionato e incondizionato producono una deviazione del normale flusso di esecuzione. Nel modello classico a cinque stadi, l’indirizzo di destinazione del salto viene determinato nello stadio di esecuzione (Execution). Tale ritardo implica che le due istruzioni che entrano nella pipeline immediatamente dopo il salto siano state già prelevate e debbano essere scartate.\nLa penalità associata corrisponde quindi a due cicli di clock per ogni salto preso. Lo stesso meccanismo si applica ai salti condizionati la cui condizione risulta vera.\n\nRiduzione della penalità di salto\nLa penalità può essere ridotta spostando al secondo stadio della pipeline (Decodifica) sia il calcolo dell’indirizzo di salto, mediante l’introduzione di un sommatore dedicato, sia la valutazione della condizione di salto, mediante il trasferimento del comparatore allo stesso stadio.\nIn questo modo, l’indirizzo di destinazione è determinato con un ciclo di anticipo rispetto al modello tradizionale, riducendo la penalità del salto a un singolo ciclo di clock.\n\nSalto differito (Delayed Branch)\nNel modello a salto differito, l’istruzione immediatamente successiva al salto viene sempre eseguita, indipendentemente dall’esito della condizione di salto. In questo contesto, l’istruzione collocata nello slot successivo al salto viene denominata delay slot.\nIl compilatore ha il compito di individuare un’istruzione semanticamente valida da collocare nel delay slot, al fine di evitare l’inserimento di istruzioni nulle (NOP). Qualora non sia possibile individuare una tale istruzione, viene inserita una NOP, mantenendo comunque una riduzione della penalità rispetto allo scarto delle istruzioni.\n\nPredizione dei salti\nLa predizione dei salti ha l’obiettivo di ridurre l’impatto dei ritardi di salto prevedendo anticipatamente l’esito di un’istruzione di salto e consentendo il prelievo dell’istruzione corretta senza attendere la valutazione della condizione.\nPredizione statica\nLa predizione statica si basa su strategie fisse, quali:\n\npresumere che il salto non venga effettuato;\npresumere che i salti all’indietro siano presi e quelli in avanti non presi;\nutilizzare un bit di predizione incorporato nell’istruzione.\n\nQuesti metodi presentano complessità ridotta ma accuratezza limitata.\nPredizione dinamica\nLa predizione dinamica utilizza il comportamento storico di ciascun salto per migliorare l’accuratezza.\nModelli tipici includono:\n\npredittori a due stati, con stati “probabilmente salta” e “probabilmente non salta”;\npredittori a quattro stati, che introducono i livelli “molto probabilmente salta” e “molto probabilmente non salta”.\n\nL’impiego dei predittori a più stati riduce la sensibilità agli errori sporadici e incrementa l’accuratezza della predizione.\n\nBuffer di destinazione di salto (Branch Target Buffer)\nPer consentire la predizione sin dallo stadio di prelievo (Fetch), è utilizzata una memoria veloce denominata Branch Target Buffer (BTB). Essa contiene per ciascun salto:\n\nl’indirizzo dell’istruzione di salto;\nuno o più bit di predizione relativi all’algoritmo adottato;\nl’indirizzo di destinazione.\n\nDurante il prelievo, se l’indirizzo dell’istruzione corrisponde a una voce nel BTB, il valore memorizzato viene utilizzato per aggiornare immediatamente il Program Counter. Qualora la tabella non possa contenere tutte le istruzioni di salto del programma, viene aggiornata dinamicamente sulla base dell’esecuzione.\n\nLimiti di risorse\nLa pipeline può andare incontro a blocchi quando più istruzioni richiedono l’uso simultaneo di una risorsa hardware condivisa. Un esempio tipico riguarda le cache unificate: durante il prelievo dell’istruzione, un accesso simultaneo alla memoria per operazioni di load o store può causare uno stallo.\nUna soluzione efficace consiste nell’adozione di cache separate per istruzioni e dati, riducendo il conflitto di accesso.\n\nValutazione delle prestazioni\nModello senza pipeline\nIl tempo di esecuzione di un programma è:\n[\nT = \\frac{N \\cdot S}{R}\n]\ndove:\n\n(N) è il numero di istruzioni,\n(S) è il numero medio di cicli per istruzione (CPI),\n(R) è la frequenza di clock.\n\nModello con pipeline\nIn condizioni ideali, il throughput della pipeline è:\nP_p = R\nossia una istruzione completata per ciclo.\nEffetto dei conflitti\nI conflitti introducono un incremento del CPI. Per ciascuna categoria di conflitto si definisce una penalità:\n\\delta = p \\cdot c\ndove:\n\n(p) è la probabilità del verificarsi del conflitto,\n(c) è il numero medio di cicli di stallo introdotti.\n\nIl CPI effettivo diventa:\nS_{tot} = S + \\delta_{dato} + \\delta_{salto} + \\delta_{miss}\n\nProcessori superscalari\nI processori superscalari possono emettere più istruzioni per ciclo, sfruttando la presenza di più unità funzionali organizzate in pipeline indipendenti.\nComponenti principali\n\nunità di prelievo con coda di istruzioni;\nunità di smistamento (dispatch) con stazioni di prenotazione;\nunità di esecuzione parallele;\nregistri temporanei;\nunità di commit con buffer di riordino.\n\nLe istruzioni possono essere inviate in esecuzione quando le risorse e gli operandi necessari risultano disponibili.\n\nDipendenze di dato e stazioni di prenotazione\nPer ogni unità di esecuzione è presente una stazione di prenotazione che contiene:\n\nl’istruzione in attesa di esecuzione,\ngli operandi o i riferimenti ai registri temporanei che li conterranno,\neventuali informazioni di controllo.\n\nUn’istruzione è resa disponibile all’esecuzione solo quando tutti gli operandi sono stati prodotti. Le unità produttori distribuiscono i risultati alle stazioni di prenotazione tramite meccanismi di inoltro dei dati.\n\nEsecuzione fuori ordine\nPer mitigare l’impatto degli stalli dovuti a cache miss o eccezioni, è consentita l’esecuzione delle istruzioni in un ordine diverso rispetto a quello del prelievo, purché ciò non alteri la semantica del programma.\nRidenominazione dei registri\nI risultati delle unità funzionali non vengono scritti direttamente nei registri architetturali ma in registri temporanei, prevenendo conflitti di nome e garantendo la correttezza delle dipendenze tra istruzioni.\nBuffer di riordino (Reorder Buffer)\nL’unità di commit trasferisce i risultati dai registri temporanei a quelli architetturali secondo l’ordine originale, garantendo precisione nelle eccezioni e consistenza dello stato del processore.\n\nPipeline nei processori CISC\nLe architetture CISC presentano sfide specifiche nella realizzazione di pipeline, dovute a:\n\ncomplessità del formato delle istruzioni,\nmolteplicità dei modi di indirizzamento,\nuso di operandi in memoria,\npresenza di flag di stato.\n\nSoluzione nei processori Intel Core i7\nI processori Intel Core i7 adottano:\n\npipeline molto profonde (14 stadi),\nemissione multipla fino a quattro istruzioni per ciclo,\ntraduzione dinamica delle istruzioni CISC in micro-operazioni interne di tipo RISC.\n\nLe micro-operazioni vengono quindi eseguite dalle unità funzionali superscalari secondo logiche di esecuzione e riordino analoghe a quelle descritte in precedenza.\n\nSTRUTTURA BASE CPU"},"Programmazione-I":{"slug":"Programmazione-I","filePath":"Programmazione I.md","title":"Programmazione I","links":[],"tags":[],"content":"gli esercizi ed i progetti si trovano su\ngithub.com/AskraZ/APPUNTI-UNICT-INF\nWHILE\nil comando while genera un ciclo che finisce quando la sua condizione (o le sue condizioni) sono soddisfatte.\n// programma che prende due variabili e ne aumenta una finché non sarà uguale\n// all&#039;altra variabile\nint main(){\nint a = 3;\nint b = 0;\nwhile (a&gt;3){\n++b;\nprintf(&quot;b è aumentato di uno&quot;);\n}\nreturn 0;\n}\nsi possono inserire anche due condizioni dentro un ciclo while:\n \nint main(){\nint a = 5;\nint b = 0;\nint c = 2;\nwhile (a&gt;b || a&gt;c){ // a&gt;b &quot;O&quot; a&gt;c, se avessimo voluto entrambe soddisfatte &quot;&amp;&amp;&quot;\n++b;\n++c;\n} // il ciclo uscirà dall&#039;esecuzione quando c=5 e b=3\n \n}\nFOR\nil ciclo for può essere definito come un ciclo definito, infatti a differenza del ciclo while sappiamo quando finirà.\nint main(){\nint eta;\nint maggiorenne = 18;\n \nfor (eta=0; eta&lt;maggiorenne; ++eta){\nprintf(&quot;%s%d%s&quot;,&quot;il ragazzo ha&quot; eta &quot;anni&quot;);\n}\n \n}\nIF\ncon if definiamo una condizione per cui qualcosa può avvenire:\nint main(){\nint eta = 18;\nint etarichiesta = 18;\n \nif (eta&gt;=etarichiesta){\n\tprintf(&quot;puoi entrare&quot;);\n\t}\nelse{\n \nprintf(&quot;non hai la giusta età per entrare&quot;);\n}\n}\nIF annidato\nint main(){\nint eta = 18;\nint etarichiesta = 18;\nint prezzoentrata = 5;\nint portafoglio = 2;\n \nif (eta&gt;=eta richiesta){\n\tprintf(&quot;sei maggiorenne, potresti entrare&quot;);\n\tif (prezzoentrata&gt;portafoglio){\n\tprintf(&quot;mi dispiace, non hai abbastanza soldi&quot;);\n\t}\n\telse {\n\tprintf(&quot;puoi entrare&quot;)\n\tportafoglio += -5;\n\t}\n}\nelse{\nprintf(&quot;devi crescere un po&#039;&quot;);\n}\nreturn 0;\n}\nSWITCH\nlo switch ti da la possibilità di configurare delle azioni in base ai “CASI”:\nint main(){\n \nint input = 0;\nprintf(&quot;inserisci un numero da 1 a 3: &quot;);\nscanf(&quot;%d&quot;, &amp;input);\n \nswitch (input){\ncase 1:\nputs(&quot;hai inserito 1&quot;);\nbreak;\ncase 2:\nputs(&quot;hai inserito 2&quot;);\nbreak;\ncase 3:\nputs(&quot;hai inserito 3&quot;);\nbreak;\n}\nreturn 0;\n}\nnon si possono usare i char nello switch\nFUNZIONI\nuna funziona può essere definita all’esterno del main per migliorare la leggibilità e la riproducibilità del software:\nint square();\nint main(){\n \nint numero;\nprintf(&quot;inserisci il numero da elevare al quadrato: &quot;)\nscanf(&quot;%d&quot;, &amp;numero);\nint numeroalquadrato= square();\nprintf(&quot;%s%d&quot;, &quot;il tuo numero al quadrato è: &quot;, numeroalquadrato);\nreturn 0;\n}\n \nint square(){\n \nnumeroalquadrato = numero * numero;\nreturn numeroalquadrato;\n}\nle variabili definite all’interno di una funzione sono disponibili solo all’interno di essa.\nArray\nun array è un gruppo di elementi immagazzinati in modo contiguo nella memoria.\n\nper fare riferimento ad una posizione, indichiamo il nome dell’array con l’indice contenuto tra parentesi quadre:\nint array[]={1,2,3,4,5};\n \narray[2]= 4; // cambiamo l&#039;elemento di indice 2 con il valore 4\nun array di tipo char può memorizzare una stringa di caratteri.\nArray bi-dimensionali\na[i][j] → elemento di riga i e di colonna j\na[2][2]={1,2}{3,4} \n\\begin{vmatrix}1 &amp; 2 \\\\ 3 &amp; 4\\end{vmatrix}\n\nPuntatori\ni puntatori sono delle variabili.\nun puntatore contiene un indirizzo di memoria.\nint a = 3;\n&amp;a // indirizzo memoria di a\nint *ptr; // allocazione di memoria per ptr\nptr = null; //definiamo che il puntatore non punta a nulla\nptr = a;\nstampa *ptr = 3;\nstampa ptr = indirizzo di memoria di a;\n \nConst"},"Strutture-Discrete":{"slug":"Strutture-Discrete","filePath":"Strutture Discrete.md","title":"Strutture Discrete","links":["tags/ESEMPI"],"tags":["ESEMPI"],"content":"Indice\nParte I\nParte II\nParte I\nLOGICA PROPOSIZIONALE\nla logica proposizionale è strettamente collegata al mondo binario costituito da {0,1}.\n\nil vocabolario consiste di un insieme di variabili proposizionali P di solito denotate con p,q,r con eventuale numerazione sottoscritta, ovvero p_{1},p_{2}.\nle variabili proposizionali possono assumere solo 2 valori: 1 o 0, rispettivamente Vero e Falso.\nle formule più complesse si possono costruire tramite i connettivi logici:\n\\neg \\rightarrow è la negazione\n\\wedge \\rightarrow è la congiunzione (e)\n\\vee  \\rightarrow è la disgiunzione (o)\n\\implies \\rightarrow è l’implicazione (se … allora …)\n\\iff \\rightarrow è la co-implicazione (se e solo se)\n\nESEMPI DI FORMULE\n\\neg P \\rightarrow non P\nP \\vee Q \\rightarrow P o Q\nP \\wedge Q \\rightarrow P e Q\nP \\implies Q \\rightarrow se P allora Q\nP \\iff Q \\rightarrow P se e solo se Q\n\n\\neg ha la precedenza sugli altri connettivi, \\vee e \\wedge hanno la stessa precedenza\n\nuna interpretazione I su P è una funzione  I:P\\rightarrow \\{0,1\\}.\ndate due formule P_{1} e P_{2} e data una interpretazione I che assegna i valori di verità alle variabili:\n\nI(\\neg P_{1}) è vera se e solo se I(P_{1}) è falsa\nI(P_{1}\\vee P_{2}) è vera se e solo se almeno una tra I(P_{1}) e I(P_{2}) è vera\nI(P_{1}\\wedge P_{2}) è vera se e solo se I(P_{1}) e I(P_{2}) sono entrambe vere\nI(P\\implies P_{2}) è vera se I(P_{1}) è falsa oppure se I(P_{2}) è vera\nI(P_{1}\\iff P_{2}) è vera se e solo se I(P_{1}\\implies P_{2}) è vera e I(P_{2}\\implies P_{1}) è pure vera\n\n\nUna formula P si dice soddisfacibile se esiste una interpretazione I delle variabili proposizionali tale che la formula risulti vera.\np\\vee(q\\wedge\\neg p)\nUna formula P si dice insoddisfacibile se per ogni interpretazione I delle variabili proposizionali non esiste nessuna interpretazione tale che la formula sia vera.\np\\wedge(q\\vee \\neg p)\\wedge(\\neg q\\wedge \\neg p)\nuna formula P si dice tautologia se per ogni interpretazione I delle variabili proposizionali la formula risulterà sempre vera.\n\nPROPRIETA’ CONNETTIVI LOGICI\n\np\\vee q\\equiv q\\vee p COMMUTATIVITA’\np\\wedge q\\equiv q\\wedge p COMMUTATIVITA’\np\\vee(q\\vee r)\\equiv (p\\vee q)\\vee r ASSOCIATIVITA’\np\\wedge(q\\wedge r)\\equiv(p\\wedge q)\\wedge r ASSOCIATIVITA’\nALTRE EQUIVALENZE LOGICHE\n\\neg (\\neg p)=p\np\\implies q\\equiv \\neg q\\implies \\neg p\np\\implies q\\equiv \\neg p\\vee q\np\\iff q\\equiv(p\\implies q)\\wedge(q\\implies p)\nLEGGI DI DE MORGAN\n\n\\neg(p\\vee q)\\equiv \\neg p\\wedge \\neg q\n\\neg(p\\wedge q)\\equiv \\neg p\\vee \\neg q\n\n\np\\wedge(q\\vee r)\\equiv(p\\vee q)\\wedge(p\\vee r)\np\\vee(q\\wedge r)\\equiv(p\\vee q)\\wedge(p\\vee r)\nGIUSTIFICAZIONE LOGICA\nsia P un insieme di proposizioni e P una proposizione.\ndiciamo che P giustifica P e lo denotiamo con P\\models P\nse ogni interpretazione I che soddisfa tutte le formule di P soddisfa anche P.\nESEMPIO\nsia P=\\{p,p\\implies q\\} allora P\\models q\n\n\ndimostriamo che se P=\\{p\\vee r,q\\vee \\neg r\\} allora P\\models p\\vee q\nCNF\nuna formula p è in forma normale congiuntiva (CNF) se è scritta come congiunzione di disgiunzioni, per esempio (p\\wedge q)\\vee(\\neg p\\wedge \\neg r\\wedge s)\nDNF\nuna formula p è in forma normale disgiuntiva (DNF) se è scritta come disgiunzione di congiunzioni, per esempio (p\\wedge q)\\vee(\\neg p\\wedge \\neg r\\wedge s)\n\n\n\nINSIEMI\nun insieme è univocamente caratterizzato dal suo contenuto, ovvero dagli elementi che gli appartengono.\n\nAPPARTENENZA\nse T è un insieme, l’espressione x\\in T si legge “x appartiene a T” oppure “x è un elemento di T”\n\n\nNON APPARTENENZA\nscriveremo invece x\\not\\in T per negare l’espressione precedente, cioè per affermare che x non appartiene a T\n\ndal momento che un insieme è caratterizzato dal suo contenuto, due insiemi che contengono gli stessi elementi, sono lo stesso insieme.\n\nDEFINIZIONE (Uguaglianza tra Insiemi)\nDue insiemi A e B sono uguali se hanno gli stessi elementi:\nA=B\\iff(\\forall x)(x\\in A\\iff x\\in B)\n\n\nquando è possibile, si può definire un insieme elencando semplicemente i suoi elementi:\n\\{1,2,3\\}= l’insieme i cui elementi sono 1,2,3\n\n\nSINGOLETTO\nla notazione \\{a\\} indica l’insieme costituito dal solo elemento a, detto singoletto\n\n\nINSIEME VUOTO\ncon il simbolo \\emptyset indicheremo l’insieme vuoto, senza nessun elemento\n\n\nUGUAGLIANZA\ndalla definizione di uguaglianza  possiamo dedurre che se A=\\{1,2,3\\} e B=\\{2,1,2,3,1\\} possiamo definirli uguali perché contengono gli stessi elementi\n\npossiamo definire un insieme anche esplicitando la proprietà che caratterizza i suoi elementi:\n\\{x:\\text{la proprietà P è vera per x}\\}\\equiv\\{x:P(x)\\} per ogni valore di x, P(x) può assumere i valori vero e falso.\nCARDINALITA’\nAd ogni insieme si può associare un’importante caratterizzazione: la quantità degli elementi che gli appartengono.\n\nDEFINIZIONE (CARDINALITA’)\ndato un insieme A, il numero di elementi che lo costituisce è denominato cardinalità dell’insieme ed è denotata con |A|.\nse |A| è un numero intero l’insieme si dice finito, altrimenti infinito.\n\nESEMPIO\nla cardinalità dell’insieme vuoto è quindi zero, |\\emptyset|=0 , mentre |\\{1,2,3\\}|=3 e |\\{a\\}|=1.\nla cardinalità dell’insieme |\\{x:x\\text{ è un numero pari}\\}|=+\\infty\nINCLUSIONE\nse abbiamo A e B e tutti gli elementi di A sono elementi di B, allora diciamo che A è incluso in B, ovvero che A è un sottoinsieme di B.\n\nDEFINIZIONE (INCLUSIONE)\nA è un sottinsieme di B, denotato con A\\subseteq B se (\\forall x)(x\\in A\\implies x\\in B)\nin questo caso possiamo anche definire B come sovrainsieme di  B\\supseteq A\n\nINSIEMI DISCRETI\n\nDEFINIZIONE (Insieme Discreto)\nUn insieme A si dice discreto se è possibili ordinare i suoi elementi in maniera tale che tra un qualunque elemento ed il successivo non vi siano altri elementi nell’insieme.\n\nOPERAZIONI TRA INSIEMI\nUNIONE\nl’unione di due insiemi A e B è l’insieme formato da:\nA\\cup B=\\{x:x\\in A\\vee x\\in B\\}\n\nA\\cup B=B\\cup A \\rightarrow COMMUTATIVITA’\nA\\cup (B\\cup C)=(A\\cup B)\\cup C \\rightarrow ASSOCIATIVITA’\n\nINTERSEZIONE\nl’intersezione di due insiemi A e B è l’insieme formato da quegli elementi che appartengono ad entrambi gli insiemi A e B:\nA\\cap B=\\{x:x\\in A\\wedge x\\in B\n\nA\\cap B= \\emptyset \\rightarrow insiemi disgiunti\nA\\cap B=B\\cap A \\rightarrow COMMUTATIVITA’\nA\\cap (B\\cap C)=(A\\cap B)\\cap C \\rightarrow ASSOCIATIVITA’\n\nCARDINALITA’ DELL’UNIONE\nse A e B sono insiemi finiti allora:\n|A\\cup B|=|A|+|B|-|A\\cap B|\nse sono disgiunti:\n|A\\cup B|=|A|+|B|\nALTRE PROPRIETA’\n\nA\\cap A=A e A\\cup A=A\nPROPRIETA’ DISTRIBUTIVA\n\nA\\cap(B\\cup C)=(A\\cap B)\\cup(A\\cap C)\nA\\cup(B\\cap C)=(A\\cup B)\\cap(A\\cup C)\n\n\nPROPRIETA’ DI ASSORBIMENTO\n\nA\\cap(A\\cup B)=A\nA\\cap(A\\cup B)=A\n\n\n\nDIFFERENZA DI INSIEMI\nla differenza di due insiemi A e B denotata con A /B è l’insieme:\nA / B=\\{x:x\\in A\\land x\\not\\in B\\}\nCARDINALITA’ DELLA DIFFERENZA\n|A /B|=|A|-|A\\cap B|\nINSIEME UNIVERSO\ndefiniamo un insieme U tale che sia un sovrainsieme di tutti gli insiemi, il complemento di un insieme è l’insieme di tutti gli elementi che non appartengono a quell’insieme.\nnotazione\nU /A=A^{c}\\rightarrow complemento  di A.\nESEMPIO\n\nse A=\\{1,2,3\\} e U=\\{1,2,3,\\dots,10\\}\nA^{c}=\\{4,5,\\dots,10\\}\n\nPROPRIETA’\n\n(A^c)^c=A\n(A\\cap B)^c=A^c\\cup B^c\n(A\\cup B)^c=A^c\\cap B^c\n\n\nCARDINALITA’ DEL COMPLEMENTO\nse U è finito e A\\subseteq U allora |A^c|=|U|-|A|\n\nDIFFERENZA SIMMETRICA\nla differenza simmetrica è l’insieme degli elementi che non appartengono ad entrambi gli insiemi:\nA\\Delta B=(A / B)\\cup(B / A)\n\nESEMPIO\nse A=\\{1,2,3\\} e B=\\{3,4,5\\}\nA\\Delta B=\\{1,2,4,5\\}\n\n\nCOMMUTATIVA\nASSOCIATIVA\n\nCARDINALITA’ DIFFERENZA SIMMETRICA\n\n|A\\Delta B|=|(A / B)\\cup(B / A)|=|A / B|+|B / A|=|A|-|A\\cap B|+|B|-|B\\cap A|=|A|+|B|-2|A\\cap B|\n\nFAMIGLIE DI INSIEMI\ndato un insieme T, consideriamo un insieme i cui elementi sono tutte le parti o sottoinsiemi di T.\nnotazione\nscriveremo questo insieme come pow(t)\n\nESEMPIO\nsia T=\\{1,2,3\\}\npow(T)=\\{\\emptyset,1,2,3,(1,2),(1,3),(2,3),(1,2,3)\\}\n\ngli elementi dell’insieme delle parti sono 2^n  (n=elementi)\n\nuna famiglia di insiemi che ha un numero infinito di elementi è una famiglia infinita.\nse invece ha un numero finito di elementi allora è una famiglia finita.\n\nESEMPIO\nsia F=\\{P,D\\} dove P è l’insieme dei numeri pari (infinito) e D l’insieme dei numeri dispari (infinito).\nla famiglia F è una famiglia finita.\nESEMPIO\nsia F=\\{P_{1},P_{2},P_{3},\\dots\\} dove P_{i}=\\{2^{1},\\dots,2^n\\}\nla famiglia è infinita ma tutti i suoi elementi sono insiemi finiti.\n\nanalogamente, se F è una famiglia qualunque di insiemi, si indica con:\n\\bigcap_{x\\in\\mathcal{F}} X\nl’insieme costituito dagli elementi che appartengono a tutti gli insiemi x\\in \\mathcal{F} e viene detto insieme intersezione della famiglia \\mathcal{F} :\n\\bigcap_{x\\in\\mathcal{F}}X=\\{x:\\forall X\\in\\mathcal{F, x\\in X}\\}\nsia dato un insieme U ed una operazione definita su uno o più elementi di U.\nse l’operazione può essere definita all’interno di U allora diciamo che U è chiuso rispetto a tale operazione.\nESEMPI\n\nsia U=N se consideriamo la somma, N è chiuso rispetto alla somma.\nse consideriamo la sottrazione, N non è chiuso rispetto alla sottrazione.\n\n\nX=\\{1,2,3\\}  consideriamo l’operazione mcm\\{1,2,3\\} , X non è chiuso rispetto a mcm perché mcm(2,3)=6\n\n\nDEFINIZIONE\nsia F una famiglia di insiemi\n\ndiciamo che F è chiusa rispetto all’unione se per ogni coppia di insiemi X e Y appartenenti a F anche X\\cup Y appartiene a F\ndiciamo che F è chiusa rispetto all’intersezione se per ogni coppia di insiemi X e Y appartenenti a F anche X\\cap Y appartiene a F\n\n\nESEMPI\n\nSia F=\\{\\{1,2,3\\},\\{1,2\\},\\{1,3\\}\\} abbiamo che F è chiuso rispetto all’unione ma non è chiuso rispetto all’intersezione\nperché \\{1,2\\}\\cap\\{1,3\\}=\\{1\\}\\not\\in F\n\nsia F una famiglia di insiemi, tutti sottinsiemi di un insieme universo U.\nquindi, F\\subseteq pow(U).\nin particolare, per ogni X\\in F , X^{c}= U / X è anche un elemento di pow(U).\npossiamo allora definire la famiglia “complemento” rispetto ad U, e la denotiamo con F^{c} come segue:\nF^{c}=\\{X^{c}:X\\in F\\}\ne notiamo (F^{c})^{c}=F\n\nTEOREMA\nla famiglia F è chiusa rispetto all’unione (risp. intersezione) \\iff la famiglia F^c è chiusa rispetto all’intersezione (risp. unione)\n#DIMOSTRAZIONE\nse F è chiusa rispetto all’unione.\nsiano X,Y\\in F^{c}. esistono allora A,B\\in F tali che  X=A^{c} e Y=B^{c} .\n\nParte II\nsull’insieme N sono definite due operazioni:\n\nSomma (+): N \\times N\\rightarrow N che ad ogni coppia di numeri (n,m) associa il numero n+m\\in N\nProdotto(\\cdot): N\\times N\\rightarrow N che ad ogni coppia di numeri (n,m) associa il numero n\\cdot m\\in N\nsull’insieme Z sono definite tre operazioni\nSomma e Prodotto: già definite in N\nDifferenza(-): Z\\times Z\\rightarrow Z che ad ogni coppia di numeri (n,m) associa il numero n-m\\in Z\n\n\nVALORE ASSOLUTO\nil valore assoluto di un intero relativo n\\in Z è l’intero |n|\\geq 0 definito come |n|=\\begin{cases}\nn,  &amp; se \\ n\\geq 0 \\\\\n-n,  &amp; se \\ n&lt;0\n\\end{cases}\n\nALCUNE PROPRIETA’\nper ogni n,m\\in Z abbiamo:\n\n|n|=0 \\iff n=0\n|n\\cdot m|=|n|\\cdot|m|\nn+|n|\\geq 0 ed in particolare n+|n|=0 \\iff n\\leq 0.\n\nDEFINIZIONE ASSIOMATICA N\n\nesiste un numero naturale 0\nogni numero naturale a ha un numero naturale successore, denotato come S(a)\nnon esiste un numero naturale il cui successore è 0\nnumeri naturali distinti hanno successori distinti: se a\\neq b\\implies S(a)\\neq S(b)\n\n\nNOTA\navendo definito la funzione + abbiamo che:\n- S(0)=1\n- per ogni numero naturale a,\\ S(a)=a+1\n- infine, dato un numero naturale n ed il suo successore S(n) diciamo che n è il predecessore di S(n) , denotato con P(S(n)).\n- Quindi, ogni numero naturale n, tranne lo 0, ha un predecessore che è il numero n-1\n\nASSIOMA DEL BUON ORDINAMENTO\n\nutilizzando la funzione successore, possiamo, come sappiamo, introdurre una relazione d’ordinamento sui numeri naturali &lt;(\\leq) definita, per ogni coppia a,b\\in N come\n\na\\le a \\\\ a&lt;S(a) \\\\\na&lt;b \\text{ se esiste} \\ c\\in N \\ \\text{tale che} \\ a&lt;c \\wedge c&lt;b\n\\end{cases}$$\n&gt; ASSIOMA DEL BUON ORDINAMENTO\n&gt; se $S$ è un qualunque insieme non vuoto di numeri naturali, allora in $S$ esiste un elemento minimo, ovvero esiste $s\\in S$ tale che $s\\le t$ per ogni $t\\in S$ \n\n**PROPRIETA&#039;** ($Z$)\nSOMMA  \n- $a+b=b+a$\n- $a+b+c=a+(b+c)=(a+b)+c$\n- $a+0=0+a=a$\nPRODOTTO\n- $a\\cdot b=b\\cdot a$ \n- $a\\cdot b\\cdot c=a\\cdot(b\\cdot c)=(a\\cdot b)\\cdot c$\n- $a\\cdot(b+c)=(a\\cdot b)+(a\\cdot c)$\n- $a\\cdot 1=1\\cdot a=a$\n- $a\\cdot 0=0\\cdot a=0$\n\n#### PRINCIPIO DI INDUZIONE\n- se una proprietà $P$ è posseduta dal numero $0$ e la proprietà $P$ è posseduta anche dal successore di ogni numero naturale che possiede la proprietà $P$, allora la proprietà $P$ è posseduta da tutti i numeri naturali\n&gt; **TEOREMA**\n\tsia $P$ una affermazione riguardante i numeri naturali. se\n\t(a). $P(0)$ è vera, ed inoltre\n\t(b). per ogni numero naturale n se $P(n)$ è vera allora è vera anche $P(n+1)$\n\t\n#DIMOSTRAZIONE \nragioniamo per assurdo e supponiamo falsa la tesi, ossia supponiamo che esista almeno un numero naturale $n$ per cui $P(n)$ è falsa. \ncostruiamo l&#039;insieme:\n$$S=\\{n:n\\in N, e \\ P(n)\\ è\\ falsa\\}$$\nper la nostra ipotesi di assurdo $S$ non è vuoto. \nPer l&#039;assioma del Buon Ordinamento esiste in $S$ un elemento minimo $s$.\nper definizione di $S$ , $P(s)$ è falsa."},"index":{"slug":"index","filePath":"index.md","title":"Hub","links":["Architettura-Degli-Elaboratori","Algebra-Lineare","Strutture-Discrete","Programmazione-I"],"tags":[],"content":"Questo è un Hub dove pubblico tutti i miei appunti scritti su Obsidian.\nArchitettura Degli Elaboratori [1 semestre] 9 CFU\nAlgebra Lineare [1 Semestre] 6 CFU\nStrutture Discrete [1 Semestre] 6 CFU\nProgrammazione I [1 Semestre] 9 CFU"}}